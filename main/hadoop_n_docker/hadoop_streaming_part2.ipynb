{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hadoop_streaming_part2.ipynb","provenance":[],"collapsed_sections":["bYnjdjkugjuF"],"authorship_tag":"ABX9TyMKTSzGfmlpT0eQuLMIqSVS"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bYnjdjkugjuF"},"source":["# Before Start!"]},{"cell_type":"markdown","metadata":{"id":"YRFiuamXVlLt"},"source":["first start with connecting the local runtime."]},{"cell_type":"code","metadata":{"id":"okNsFAEBVMJH","executionInfo":{"status":"ok","timestamp":1604075715586,"user_tz":420,"elapsed":190,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"628a0604-db6e-4662-b616-e64cf6a7de64","colab":{"base_uri":"https://localhost:8080/"}},"source":["!docker ps -a "],"execution_count":1,"outputs":[{"output_type":"stream","text":["CONTAINER ID        IMAGE                             COMMAND                  CREATED             STATUS                     PORTS               NAMES\r\n","8a555e709777        sakha002/pyprotobuf2:latest       \"/bin/bash\"              10 days ago         Exited (0) 9 days ago                          determined_williamson\r\n","03d84bc5c6a9        sakha002/historyserver:latest     \"/entrypoint.sh /run…\"   2 weeks ago         Exited (137) 2 weeks ago                       historyserver\r\n","7f21947fc924        sakha002/nodemanager:latest       \"/entrypoint.sh /run…\"   2 weeks ago         Exited (137) 2 weeks ago                       nodemanager1\r\n","879d3c554959        sakha002/resourcemanager:latest   \"/entrypoint.sh /run…\"   2 weeks ago         Exited (137) 2 weeks ago                       resourcemanager\r\n","05f6f655c88d        sakha002/datanode:latest          \"/entrypoint.sh /run…\"   2 weeks ago         Exited (137) 2 weeks ago                       datanode3\r\n","70402d515bdb        sakha002/datanode:latest          \"/entrypoint.sh /run…\"   2 weeks ago         Exited (137) 2 weeks ago                       datanode1\r\n","d0da94ad0f4a        sakha002/datanode:latest          \"/entrypoint.sh /run…\"   2 weeks ago         Exited (137) 2 weeks ago                       datanode2\r\n","8a73b54e28e5        sakha002/namenode:latest          \"/entrypoint.sh /run…\"   2 weeks ago         Exited (137) 2 weeks ago                       namenode\r\n","531e654272af        nginx                             \"/docker-entrypoint.…\"   3 weeks ago         Exited (0) 3 weeks ago                         myserver\r\n","8641e0a64e03        mytraps:demo                      \"/bin/sh /root/traps\"    3 weeks ago         Exited (0) 3 weeks ago                         mycontain\r\n","f6e8281f6f27        mysql:5.7                         \"docker-entrypoint.s…\"   3 weeks ago         Exited (0) 3 weeks ago                         app_mysql_1\r\n","d2352454e66f        node:12-alpine                    \"docker-entrypoint.s…\"   3 weeks ago         Exited (1) 3 weeks ago                         app_app_1\r\n","bb07ab843c4d        node:12-alpine                    \"docker-entrypoint.s…\"   3 weeks ago         Created                                        competent_kowalevski\r\n","4493421d5a28        nicolaka/netshoot                 \"/bin/bash -l\"           3 weeks ago         Exited (0) 3 weeks ago                         intelligent_pike\r\n","658bed8ba63f        docker101tutorial                 \"/docker-entrypoint.…\"   3 weeks ago         Exited (0) 3 weeks ago                         docker-tutorial2\r\n","11c586eda298        alpine/git                        \"git --help\"             3 weeks ago         Exited (0) 3 weeks ago                         hossein_repo6\r\n","b5f82006a8af        busybox                           \"echo 'hello from bu…\"   4 weeks ago         Exited (0) 4 weeks ago                         elated_clarke\r\n","1c45cf2e260d        busybox                           \"sh\"                     8 weeks ago         Exited (137) 8 weeks ago                       elated_cray\r\n","ab321a8b3b42        hello-world:latest                \"/hello\"                 8 weeks ago         Exited (0) 8 weeks ago                         blissful_kilby\r\n","ff53f58c645e        hello-world                       \"/hello\"                 3 months ago        Exited (0) 3 months ago                        crazy_herschel\r\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"58pgu2MmVjnL"},"source":["okay some clean up of containers\n","Q:  when a container exits, can we resume it and continue where we left off? should be .."]},{"cell_type":"code","metadata":{"id":"COQWinIrYWmg","executionInfo":{"status":"ok","timestamp":1604076071019,"user_tz":420,"elapsed":214,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"505ce3f5-6cc4-4b27-b6ad-aa90a88ab76c","colab":{"base_uri":"https://localhost:8080/"}},"source":["!docker container prune "],"execution_count":4,"outputs":[{"output_type":"stream","text":["unknown shorthand flag: 'y' in -y\r\n","See 'docker container prune --help'.\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PJ6wKMbZYtDV","executionInfo":{"status":"ok","timestamp":1604076112256,"user_tz":420,"elapsed":645,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"100205a1-235c-4043-9183-727d69b8cc8a","colab":{"base_uri":"https://localhost:8080/"}},"source":["!docker container prune -f"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Deleted Containers:\r\n","8a555e7097775839b8d6a3ddd31048b73b8ff94435e793dff50a69d5f7526a12\r\n","03d84bc5c6a9ee97c43e2eba85b855c356745991f93d7e7b82dce6026713d5a9\r\n","7f21947fc924301c495848071d2606673d68b686cef47bea445cedacaad8d256\r\n","879d3c5549594c5064a2a75d719acf10932186306cdf9c602706fb333816ad15\r\n","05f6f655c88ddb5e25dce55f0fe0247aca2a5e7559dafbad90c03cf75e868d7a\r\n","70402d515bdb0af0a780092119253d91ab898b4176baa9ce865e7acc067b7ba5\r\n","d0da94ad0f4a03a49e5e13d2930b7a8af1c0765ea1666a2bc1a6290a8256049c\r\n","8a73b54e28e53ec30a278ed799b91979160c26c49b8a9be451dc2ccd35ac5081\r\n","531e654272af1052a125b6f764d5a34d7e1df1f219c7b274dcaa0c9a248a96a5\r\n","8641e0a64e033f301dfc2f565cd3bd0d9ebf87e042ca9c9a2131b002c04bc3cc\r\n","f6e8281f6f27dcb86cc7190d4d3be63067fdb510fa1965033d649389b3a07c24\r\n","d2352454e66f5619f387c287f84bd0b09d59a775af33a5609c5e671987c51ae8\r\n","bb07ab843c4d0d10921f9c20fcf2aa7c9fa8a8bfbd23274763ec8b9abebb0629\r\n","4493421d5a28a6907371b45ca42f18c9116304373a35f2b38f920f8cdba339a8\r\n","658bed8ba63f7fe734c94b46acd68f4dbe0a3578bd69058a8d1aa47330992084\r\n","11c586eda298bfd82e7e900d51199f7464cf9302ca96ff9eb5672720d556bef9\r\n","b5f82006a8affc990d620144ba69028f38b3c3f41fb8c9b6594610cc4083489c\r\n","1c45cf2e260d15cde0f3a703799112d5a7686357114d2798c178f9631f0f181f\r\n","ab321a8b3b422deb516da0c4cd0136e982294d298b526eda34587951e20f3c2f\r\n","ff53f58c645edd806be5336af597c4ccbe687df1f678fd18af44954015c71d12\r\n","\r\n","Total reclaimed space: 4.994MB\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q3tcRAA_YzIO","executionInfo":{"status":"ok","timestamp":1604076133990,"user_tz":420,"elapsed":173,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"a9723eef-eee0-45f3-e1cc-201b7123e341","colab":{"base_uri":"https://localhost:8080/"}},"source":["!docker ps -a"],"execution_count":6,"outputs":[{"output_type":"stream","text":["CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\r\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kOgn7hLoY2xV"},"source":["Now getting back to the hadoop containers. \n","where are they?"]},{"cell_type":"code","metadata":{"id":"X9RDi-z2ZCXB","executionInfo":{"status":"ok","timestamp":1604076208613,"user_tz":420,"elapsed":122,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"dcad9b81-e400-4d00-9ac0-06ccd0e85187","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Desktop    Downloads  localperl  Pictures  snap       Videos\r\n","Documents  gopath     Music\t Public    Templates\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k2rMgPGZZI4e","executionInfo":{"status":"ok","timestamp":1604076273690,"user_tz":420,"elapsed":134,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"901e109e-a39c-4345-de51-45a29a83d276","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd ../../media/hossein/HDD/gdrive/Active_projects/my_dfs/\n","!ls"],"execution_count":8,"outputs":[{"output_type":"stream","text":["/media/hossein/HDD/gdrive/Active_projects/my_dfs\n","docker-hadoop\t\t\t\t     protobuf\r\n","hadoop-mapreduce-examples-2.7.1-sources.jar  protobuf-python-3.13.0.tar.gz\r\n","hadoop_source\t\t\t\t     python_hadoop_examples\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N8pUQuTkaKmK","executionInfo":{"status":"ok","timestamp":1604076535884,"user_tz":420,"elapsed":136,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"c2df2682-8a8e-43ac-b2f4-5f4df00ed8c6","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd ../../SkillsLearningTraining/docker-tutorials/main/hadoop_n_docker/my_dfs/\n","!ls\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/media/hossein/HDD/gdrive/SkillsLearningTraining/docker-tutorials/main/hadoop_n_docker/my_dfs\n","docker-hadoop\t\t\t\t     hadoop_source\r\n","hadoop-mapreduce-examples-2.7.1-sources.jar  python_hadoop_examples\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UKKqsT3Dbol5","executionInfo":{"status":"ok","timestamp":1604076952566,"user_tz":420,"elapsed":140,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"a9bc796c-73c9-4558-ae5c-11e979f5fc15","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd docker-hadoop/\n","!ls"],"execution_count":10,"outputs":[{"output_type":"stream","text":["/media/hossein/HDD/gdrive/SkillsLearningTraining/docker-tutorials/main/hadoop_n_docker/my_dfs/docker-hadoop\n","base\t\t       docker-compose.yml  Makefile  nodemanager      submit\r\n","datanode\t       hadoop.env\t   namenode  README.md\r\n","docker-compose-v3.yml  historyserver\t   nginx     resourcemanager\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ptKEdpdVcFZ5","executionInfo":{"status":"ok","timestamp":1604077166619,"user_tz":420,"elapsed":3391,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"6e5f65d7-9a5e-488d-c4ee-69e799d71c81","colab":{"base_uri":"https://localhost:8080/"}},"source":["!docker-compose up -d"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Creating namenode ... \n","\u001b[1BCreating datanode3 ... \n","Creating datanode2 ... \n","Creating datanode1 ... \n","\u001b[2BCreating resourcemanager ... \n","Creating historyserver   ... \n","Creating nodemanager1    ... \n","\u001b[3B"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u2eZmWpbc26R"},"source":["okay so observation is that using this docker compose fails to properly spin up the resource manager container everytime, and I will end up doing it manually (using the vscode UI)\n","but now I guess we are generally all set for the streaming.\n"]},{"cell_type":"markdown","metadata":{"id":"u_-Y366RgJhj"},"source":["# Q: how to work with the hadoop dfs\n"," (should get this figured out)"]},{"cell_type":"markdown","metadata":{"id":"23dcCuBYe2_V"},"source":["# Back to Streaming Examples"]},{"cell_type":"markdown","metadata":{"id":"3DAuilace8AK"},"source":["okay so in the previous example, I did that well worn word count using the map reduce scripts.\n","\n","Should I remind myself of what was the map part, and what was the reduce part?\n","okay so I guess there was some more falvors of the same word count example that I can try and refresh mind too.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gIaOKOqNzhIi"},"source":["##WordCount Example (reminers)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pyvVALdXzkeO"},"source":["```\n","hduser@ubuntu:/usr/local/hadoop$ bin/hadoop dfs -copyFromLocal /tmp/gutenberg /user/hduser/gutenberg\n","hduser@ubuntu:/usr/local/hadoop$ bin/hadoop dfs -ls\n","```\n","\n","we let the **Hadoop group the (key, value) pairs** between the Map and the Reduce step because Hadoop is more efficient in this regard than our simple Python scripts."]},{"cell_type":"markdown","metadata":{"id":"n1A_ZQqb5V_7"},"source":["okay now we add the second mapper and reducrs for the Word count:\n","\n","```\n","#!/usr/bin/env python3\n","\"\"\"A more advanced Mapper, using Python iterators and generators.\"\"\"\n","\n","import sys\n","\n","def read_input(file):\n","    for line in file:\n","        # split the line into words\n","        yield line.split()\n","\n","def main(separator='\\t'):\n","    # input comes from STDIN (standard input)\n","    data = read_input(sys.stdin)\n","    for words in data:\n","        # write the results to STDOUT (standard output);\n","        # what we output here will be the input for the\n","        # Reduce step, i.e. the input for reducer.py\n","        #\n","        # tab-delimited; the trivial word count is 1\n","        for word in words:\n","            print ('%s%s%d' % (word, separator, 1))\n","\n","if __name__ == \"__main__\":\n","    main()\n","```\n","\n","\n","```\n","#!/usr/bin/env python\n","\"\"\"A more advanced Reducer, using Python iterators and generators.\"\"\"\n","\n","from itertools import groupby\n","from operator import itemgetter\n","import sys\n","\n","def read_mapper_output(file, separator='\\t'):\n","    for line in file:\n","        yield line.rstrip().split(separator, 1)\n","\n","def main(separator='\\t'):\n","    # input comes from STDIN (standard input)\n","    data = read_mapper_output(sys.stdin, separator=separator)\n","    # groupby groups multiple word-count pairs by word,\n","    # and creates an iterator that returns consecutive keys and their group:\n","    #   current_word - string containing a word (the key)\n","    #   group - iterator yielding all [\"&lt;current_word&gt;\", \"&lt;count&gt;\"] items\n","    for current_word, group in groupby(data, itemgetter(0)):\n","        try:\n","            total_count = sum(int(count) for current_word, count in group)\n","            print \"%s%s%d\" % (current_word, separator, total_count)\n","        except ValueError:\n","            # count was not a number, so silently discard this item\n","            pass\n","\n","if __name__ == \"__main__\":\n","    main()\n","\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"RzuIq4fk-7fl"},"source":["### summary of my take on mapper and reducer\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pPpcfqWG_FBh"},"source":["\n","okay so now we have our mapper and reducer.\n","so a quick look at them shows that  \n","\n","* the mapper reads the  Data from **stdin**  and generates a set of (key, values)  in **stdout** ( which is print )\n","\n","* the hadoop  groups this  (big) set by Key values ( or maybe something else, hmmm!)\n","\n","* the reducer gets the  set of  (key,value)s  from  **stdin** .  It performs some grouping based on the keys ( and maybe some computation of the values) and then produce the results for their Set to the **stdout**\n","\n","okay "]},{"cell_type":"markdown","metadata":{"id":"__BHNMrB_JDj"},"source":["### Running the mapper/reducer with streaming\n"]},{"cell_type":"markdown","metadata":{"id":"jau-R0gk_hGv"},"source":["okay checked inside my hadoop cluster name node. we have python3 there , though the version is old (3.5.3) but anyway!\n","\n","The Streaming jar file is not in there.\n","\n","I need to copy that file plus the mapper/reducer files into the name node\n","\n","\n","Also the books and all the other stuff that I have added to the hdfs before is there still.\n","```\n","root@ca146de6e692:/# hdfs dfs -ls\n","Found 15 items\n","drwxr-xr-x   - root supergroup          0 2020-10-08 20:30 daily\n","drwxr-xr-x   - root supergroup          0 2020-10-11 02:14 gutenberg\n","drwxr-xr-x   - root supergroup          0 2020-10-10 17:16 gutenberg-output\n","drwxr-xr-x   - root supergroup          0 2020-10-11 02:19 gutenberg-output11\n","drwxr-xr-x   - root supergroup          0 2020-10-11 02:33 gutenberg-output12\n","drwxr-xr-x   - root supergroup          0 2020-10-10 17:42 gutenberg-output2\n","drwxr-xr-x   - root supergroup          0 2020-10-10 17:46 gutenberg-output3\n","drwxr-xr-x   - root supergroup          0 2020-10-11 02:15 gutenberg-output4\n","drwxr-xr-x   - root supergroup          0 2020-10-11 01:23 gutenberg-output5\n","drwxr-xr-x   - root supergroup          0 2020-10-11 01:33 gutenberg-output6\n","drwxr-xr-x   - root supergroup          0 2020-10-11 01:38 gutenberg-output7\n","drwxr-xr-x   - root supergroup          0 2020-10-11 01:40 gutenberg-output8\n","drwxr-xr-x   - root supergroup          0 2020-10-11 01:56 gutenberg-output9\n","drwxr-xr-x   - root supergroup          0 2020-10-07 19:48 input\n","drwxr-xr-x   - root supergroup          0 2020-10-08 19:57 output\n","```"]},{"cell_type":"code","metadata":{"id":"rDgbL84s5VwB","executionInfo":{"status":"ok","timestamp":1604086565133,"user_tz":420,"elapsed":168,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"0fb79fd1-9b15-461f-8b4d-67bff87f8d84","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls"],"execution_count":13,"outputs":[{"output_type":"stream","text":["base\t\t       docker-compose.yml  Makefile  nodemanager      submit\r\n","datanode\t       hadoop.env\t   namenode  README.md\r\n","docker-compose-v3.yml  historyserver\t   nginx     resourcemanager\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ay__ojQK5VSB","executionInfo":{"status":"ok","timestamp":1604086594398,"user_tz":420,"elapsed":120,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"60adc9b5-159b-4b4b-f188-cd9f3b54ae92","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd ..\n","!ls"],"execution_count":14,"outputs":[{"output_type":"stream","text":["/media/hossein/HDD/gdrive/SkillsLearningTraining/docker-tutorials/main/hadoop_n_docker/my_dfs\n","docker-hadoop\t\t\t\t     hadoop_source\r\n","hadoop-mapreduce-examples-2.7.1-sources.jar  python_hadoop_examples\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NwQ5tg9we7C_","executionInfo":{"status":"ok","timestamp":1604086677818,"user_tz":420,"elapsed":151,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"4b642399-41af-4812-f806-ec59f45464b5","colab":{"base_uri":"https://localhost:8080/"}},"source":["!docker ps"],"execution_count":15,"outputs":[{"output_type":"stream","text":["CONTAINER ID        IMAGE                             COMMAND                  CREATED             STATUS                 PORTS                    NAMES\r\n","3c3fb342cbe9        sakha002/nodemanager:latest       \"/entrypoint.sh /run…\"   3 hours ago         Up 3 hours (healthy)   8042/tcp                 nodemanager1\r\n","4964d37233e1        sakha002/resourcemanager:latest   \"/entrypoint.sh /run…\"   3 hours ago         Up 3 hours (healthy)   0.0.0.0:8089->8088/tcp   resourcemanager\r\n","29f240e07a9a        sakha002/historyserver:latest     \"/entrypoint.sh /run…\"   3 hours ago         Up 3 hours (healthy)   8188/tcp                 historyserver\r\n","4842dd9e1b7b        sakha002/datanode:latest          \"/entrypoint.sh /run…\"   3 hours ago         Up 3 hours (healthy)   9864/tcp                 datanode1\r\n","0e4d8cbf0bb5        sakha002/datanode:latest          \"/entrypoint.sh /run…\"   3 hours ago         Up 3 hours (healthy)   9864/tcp                 datanode2\r\n","1e452e8cb7ab        sakha002/datanode:latest          \"/entrypoint.sh /run…\"   3 hours ago         Up 3 hours (healthy)   9864/tcp                 datanode3\r\n","ca146de6e692        sakha002/namenode:latest          \"/entrypoint.sh /run…\"   3 hours ago         Up 3 hours (healthy)   0.0.0.0:9870->9870/tcp   namenode\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J02bA-tsBEyA","executionInfo":{"status":"ok","timestamp":1604086725572,"user_tz":420,"elapsed":321,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}}},"source":["!docker cp hadoop_source/hadoop-streaming-3.1.0.jar  namenode:."],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"XHdp0kKqBUhO","executionInfo":{"status":"ok","timestamp":1604087298626,"user_tz":420,"elapsed":132,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"fe3fe6d4-fd75-406a-9ff1-4bc709c4d955","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls python_hadoop_examples/example3/\n"],"execution_count":22,"outputs":[{"output_type":"stream","text":["mapper.py  reducer.py\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sdiFQUz-Dd-1","executionInfo":{"status":"ok","timestamp":1604087553270,"user_tz":420,"elapsed":211,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}}},"source":["!docker cp python_hadoop_examples/example3/.   namenode:media/\n"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z7gKDw3bEeKn"},"source":["okay so here a docker command got me stuck for a few. Note that here I wanted to copy all files within the example3 directory to copy in my destination path.\n","\n","I first tried  example3/  which did not work and also tried  example3/* and it is not working, so after a few searches I dfound this :\n","\n","https://stackoverflow.com/questions/32566624/docker-cp-all-files-from-a-folder-to-existing-container-folder"]},{"cell_type":"markdown","metadata":{"id":"yYj2uWWzE_ZN"},"source":["Okay now I have all my files there. Only need to run the streaming command:\n","\n","\n","```\n","hadoop jar hadoop-*streaming*.jar \\\n","-mapper /media/mapper.py \\\n","-reducer /media/reducer.py \\\n","-input /root/gutenberg/*  \\\n","-output /root/gutenberg-output20 \n","\n","```\n","\n","okay so looks like a few errors:\n","\n","\n","first:\n","```\n","hadoop jar hadoop-*streaming*.jar \\\n","-mapper /media/mapper.py \\\n","-reducer /media/reducer.py \\\n","-input gutenberg/*  \\\n","-output gutenberg-output20 \n","\n","```\n","second looks like my python mapper reducer fall into error, so need to make them execuatable.\n","Okay so I made two chages \n","\n"," \n","```\n","chmod +x mapper.py \n","```\n","and for the other one and then changing this:\n","```\n","#!/usr/bin/python3\n","```\n","and finally the command should be like this\n","```\n","hadoop jar hadoop-*streaming*.jar -file /media/mapper.py -mapper /media/mapper.py -file /media/reducer.py -reducer /media/reducer.py -input gutenberg/*  -output gutenberg-output23 \n","```\n"]},{"cell_type":"code","metadata":{"id":"1X9etadIE-iH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ykjCXKBnKFHq"},"source":["## Another Example:\n","\n"]},{"cell_type":"code","metadata":{"id":"hCIdyck0q8qO","executionInfo":{"status":"ok","timestamp":1604097720933,"user_tz":420,"elapsed":263,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"aeba84ec-ac5b-4bf2-cf2b-611b4173cba0","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls\n","%cd python_hadoop_examples/\n","%mkdir example4\n","%cd example4"],"execution_count":26,"outputs":[{"output_type":"stream","text":["docker-hadoop\t\t\t\t     hadoop_source\r\n","hadoop-mapreduce-examples-2.7.1-sources.jar  python_hadoop_examples\r\n","/media/hossein/HDD/gdrive/SkillsLearningTraining/docker-tutorials/main/hadoop_n_docker/my_dfs/python_hadoop_examples\n","/media/hossein/HDD/gdrive/SkillsLearningTraining/docker-tutorials/main/hadoop_n_docker/my_dfs/python_hadoop_examples/example4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PE--d5e-rNTU","executionInfo":{"status":"ok","timestamp":1604097736166,"user_tz":420,"elapsed":4343,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"8ee7c93a-7378-44da-bf08-a589ab2c1d23","colab":{"base_uri":"https://localhost:8080/"}},"source":["!git clone https://github.com/eljefe6a/nfldata.git\n"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Cloning into 'nfldata'...\n","remote: Enumerating objects: 660, done.\u001b[K\n","remote: Total 660 (delta 0), reused 0 (delta 0), pack-reused 660\u001b[K\n","Receiving objects: 100% (660/660), 17.05 MiB | 9.33 MiB/s, done.\n","Resolving deltas: 100% (353/353), done.\n","Updating files: 100% (54/54), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E2W-ktiFrRmc","executionInfo":{"status":"ok","timestamp":1604098060844,"user_tz":420,"elapsed":387,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"d7a656ab-f507-4957-8f9c-d7f6ef0149ea","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd nfldata/\n","!cat stadiums.csv\n","!dos2unix -l -n stadiums.csv unixstadiums.csv\n","!cat unixstadiums.csv # Hooray! One stadium per line"],"execution_count":30,"outputs":[{"output_type":"stream","text":["[Errno 2] No such file or directory: 'nfldata/'\n","/media/hossein/HDD/gdrive/SkillsLearningTraining/docker-tutorials/main/hadoop_n_docker/my_dfs/python_hadoop_examples/example4/nfldata\n","University of Phoenix Stadium,63400,78600,Glendale Arizona,419 Tifway Bermuda Grass,FALSE,ARI,2006,GHCND:USW00023183,Retractable,1150dos2unix: converting file stadiums.csv to file unixstadiums.csv in Unix format...\n","University of Phoenix Stadium,63400,78600,Glendale Arizona,419 Tifway Bermuda Grass,FALSE,ARI,2006,GHCND:USW00023183,Retractable,1150"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YPsNQCYPtkry"},"source":["```\n","hdfs dfs -mkdir nfldata/\n","hdfs dfs -mkdir nfldata/stadiums\n","hdfs dfs -put /media/unixstadiums.csv  nfldata/stadiums/\n","\n","hadoop jar hadoop-*streaming*.jar \\\n","    -input nfldata/stadiums \\\n","    -output nfldata/output1 \\\n","    -mapper cat \\\n","    -reducer \"wc -l\"\n","\n","# now we check our results:\n","hadoop fs -ls nfldata/output1\n","\n","# looks like files are there, lets get the result:\n","hadoop fs -text nfldata/output1/part*\n","# => 32\n","\n","```\n"]},{"cell_type":"code","metadata":{"id":"e6IYQrE4s_EA","executionInfo":{"status":"ok","timestamp":1604098378328,"user_tz":420,"elapsed":139,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"2b6834d1-1837-4fc3-fa5e-e9eadeb44085","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls"],"execution_count":31,"outputs":[{"output_type":"stream","text":["173328.csv\t\t  drivestransform.py\t      README.md\r\n","adddriveresult.hql\t  input\t\t\t      setup.sh\r\n","adddrives.hql\t\t  license.md\t\t      spark\r\n","arrests.csv\t\t  playbyplay_join.hql\t      src\r\n","columns.txt\t\t  playbyplay_tablecreate.hql  stadiums.csv\r\n","conf\t\t\t  queries.hql\t\t      unixstadiums.csv\r\n","drivesresulttransform.py  queries.pig\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0401SSbWtt9Q","executionInfo":{"status":"ok","timestamp":1604098434925,"user_tz":420,"elapsed":213,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}}},"source":["!docker cp unixstadiums.csv namenode:media/unixstadiums.csv"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iuNHOGZbxE_Q"},"source":["```\n","#mapper\n","import sys\n","\n","for line in sys.stdin:\n","    line = line.strip()\n","    unpacked = line.split(\",\")\n","    stadium, capacity, expanded, location, surface, turf, team, opened, weather, roof, elevation = line.split(\",\")\n","    results = [turf, \"1\"]\n","    print(\"\\t\".join(results))\n","\n","\n","```\n","\n","okay so in order to get myself a bit familar on how line.split unpack, etc. works, I changed it to this which is really not that big a deal\n","\n","\n","```\n","with open (\"./unixstadiums.csv\") as file:\n","\n","    lines= file.readlines()\n","    for line in lines:\n","    \n","        line = line.strip()\n","        unpacked = line.split(\",\")\n","        stadium, capacity, expanded, location, surface, turf, team, opened, weather, roof, elevation = line.split(\",\")\n","        results = [turf, \"1\"]\n","        print(\"\\t\".join(results))`)\n","```\n","\n","```\n","#reducer\n","\n","import sys\n","\n","# Example input (ordered by key)\n","# FALSE 1\n","# FALSE 1\n","# TRUE 1\n","# TRUE 1\n","# UNKNOWN 1\n","# UNKNOWN 1\n","\n","# keys come grouped together\n","# so we need to keep track of state a little bit\n","# thus when the key changes (turf), we need to reset\n","# our counter, and write out the count we've accumulated\n","\n","last_turf = None\n","turf_count = 0\n","\n","for line in sys.stdin:\n","\n","    line = line.strip()\n","    turf, count = line.split(\"\\t\")\n","\n","    count = int(count)\n","    # if this is the first iteration\n","    if not last_turf:\n","        last_turf = turf\n","\n","    # if they're the same, log it\n","    if turf == last_turf:\n","        turf_count += count\n","    else:\n","        # state change (previous line was k=x, this line is k=y)\n","        result = [last_turf, turf_count]\n","        print(\"\\t\".join(str(v) for v in result))\n","        last_turf = turf\n","        turf_count = 1\n","\n","# this is to catch the final counts after all records have been received.\n","print(\"\\t\".join(str(v) for v in [last_turf, turf_count]))\n","\n","```"]},{"cell_type":"code","metadata":{"id":"leH_me4U4-cc","executionInfo":{"status":"ok","timestamp":1604101474294,"user_tz":420,"elapsed":252,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"5c5eb0e8-9778-435d-83cf-69226b9f6e35","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls\n","%cd gdrive/SkillsLearningTraining/docker-tutorials/main/hadoop_n_docker/my_dfs/python_hadoop_examples/example4\n","!ls"],"execution_count":39,"outputs":[{"output_type":"stream","text":[" gdrive\t\t\t      Hossein_personal\t'System Volume Information'\r\n"," homepage_extracted_fromuce   HOSSEIN_WCGEC\t Tempfiles\n","/media/hossein/HDD/gdrive/SkillsLearningTraining/docker-tutorials/main/hadoop_n_docker/my_dfs/python_hadoop_examples/example4\n","mapper.py  reducer.py  unixstadiums.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eRrbpRL85irI","executionInfo":{"status":"ok","timestamp":1604101489755,"user_tz":420,"elapsed":150,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"08b00fc5-80a0-4a25-fca8-126d3fa7663d","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls"],"execution_count":40,"outputs":[{"output_type":"stream","text":["mapper.py  reducer.py  unixstadiums.csv\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iYPRHOTE5l7l","executionInfo":{"status":"ok","timestamp":1604101573560,"user_tz":420,"elapsed":83,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"dbdb706f-061b-4dc0-afd1-6d0e6271f151","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd .."],"execution_count":41,"outputs":[{"output_type":"stream","text":["/media/hossein/HDD/gdrive/SkillsLearningTraining/docker-tutorials/main/hadoop_n_docker/my_dfs/python_hadoop_examples\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sNaHF6DH57hi","executionInfo":{"status":"ok","timestamp":1604101668052,"user_tz":420,"elapsed":202,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}}},"source":["!docker cp ./example4/. namenode:media/"],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wCWxepKb6ahd"},"source":["```\n","hadoop jar hadoop-*streaming*.jar \\\n","    -file media/mapper.py \\\n","    -mapper media/mapper.py \\\n","    -file media/reducer.py \\\n","    -reducer media/reducer.py \\\n","    -input nfldata/stadiums \\\n","    -output nfldata/pythonoutput \\   \n","\n","```\n","\n","```\n","hadoop fs -text nfldata/pythonoutput/part-*\n","FALSE 15\n","TRUE 17\n","```"]}]}