{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hadoop_streaming_in_container.ipynb","provenance":[],"collapsed_sections":["PvCM-5qdpCao","U1L9e516ZnZn","jZwusdYSlQDr","g7AjLTK8a3Ev","vDu1Nyi14SSM","i7pNhUuMrAeb","m4BWPXVP-Dzo","5aSWc3zTq8V4","MpKC_QgFg9H_","g6gUCuDWq8eW"],"authorship_tag":"ABX9TyM/vLNvXKWO97Z72/hnnAx6"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"IuVQyDGQ7T5X","executionInfo":{"status":"ok","timestamp":1602358820419,"user_tz":420,"elapsed":162,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"89554260-b17d-4f38-80a0-2f263f0a84f2","colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["!docker ps -a"],"execution_count":1,"outputs":[{"output_type":"stream","text":["CONTAINER ID        IMAGE                                                    COMMAND                  CREATED             STATUS                     PORTS                    NAMES\r\n","78cd0dd5344e        bde2020/hadoop-nodemanager:1.1.0-hadoop2.7.1-java8       \"/entrypoint.sh /run…\"   16 hours ago        Up 16 hours (healthy)      8042/tcp                 nodemanager1\r\n","f9b4ef7c4437        bde2020/hadoop-resourcemanager:1.1.0-hadoop2.7.1-java8   \"/entrypoint.sh /run…\"   16 hours ago        Up 16 hours (healthy)      0.0.0.0:8089->8088/tcp   resourcemanager\r\n","241cc6651043        sakha002/namenode:latest                                 \"/entrypoint.sh /run…\"   16 hours ago        Up 16 hours (healthy)      0.0.0.0:9870->9870/tcp   namenode\r\n","b61fbbd6c982        bde2020/hadoop-historyserver:1.1.0-hadoop2.7.1-java8     \"/entrypoint.sh /run…\"   17 hours ago        Up 17 hours (healthy)      8188/tcp                 historyserver\r\n","e62f0db47c80        bde2020/hadoop-datanode:1.1.0-hadoop2.7.1-java8          \"/entrypoint.sh /run…\"   17 hours ago        Up 17 hours (healthy)      9864/tcp                 datanode2\r\n","9c871ccb3d65        bde2020/hadoop-datanode:1.1.0-hadoop2.7.1-java8          \"/entrypoint.sh /run…\"   17 hours ago        Up 17 hours (healthy)      9864/tcp                 datanode1\r\n","1581fd68898f        bde2020/hadoop-datanode:1.1.0-hadoop2.7.1-java8          \"/entrypoint.sh /run…\"   17 hours ago        Up 17 hours (healthy)      9864/tcp                 datanode3\r\n","531e654272af        nginx                                                    \"/docker-entrypoint.…\"   3 days ago          Exited (0) 3 days ago                               myserver\r\n","8641e0a64e03        mytraps:demo                                             \"/bin/sh /root/traps\"    3 days ago          Exited (0) 3 days ago                               mycontain\r\n","f6e8281f6f27        mysql:5.7                                                \"docker-entrypoint.s…\"   4 days ago          Exited (0) 3 days ago                               app_mysql_1\r\n","d2352454e66f        node:12-alpine                                           \"docker-entrypoint.s…\"   4 days ago          Exited (1) 3 days ago                               app_app_1\r\n","bb07ab843c4d        node:12-alpine                                           \"docker-entrypoint.s…\"   4 days ago          Created                                             competent_kowalevski\r\n","4493421d5a28        nicolaka/netshoot                                        \"/bin/bash -l\"           4 days ago          Exited (0) 4 days ago                               intelligent_pike\r\n","658bed8ba63f        docker101tutorial                                        \"/docker-entrypoint.…\"   8 days ago          Exited (0) 3 days ago                               docker-tutorial2\r\n","11c586eda298        alpine/git                                               \"git --help\"             8 days ago          Exited (0) 8 days ago                               hossein_repo6\r\n","b5f82006a8af        busybox                                                  \"echo 'hello from bu…\"   8 days ago          Exited (0) 8 days ago                               elated_clarke\r\n","1c45cf2e260d        busybox                                                  \"sh\"                     5 weeks ago         Exited (137) 5 weeks ago                            elated_cray\r\n","ab321a8b3b42        hello-world:latest                                       \"/hello\"                 5 weeks ago         Exited (0) 5 weeks ago                              blissful_kilby\r\n","ff53f58c645e        hello-world                                              \"/hello\"                 2 months ago        Exited (0) 2 months ago                             crazy_herschel\r\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PvCM-5qdpCao"},"source":["# Using Hadoop Streaming in container"]},{"cell_type":"markdown","metadata":{"id":"nVyuHab1uv_5"},"source":["https://medium.com/@rrfd/your-first-map-reduce-using-hadoop-with-python-and-osx-ca3b6f3dfe78"]},{"cell_type":"markdown","metadata":{"id":"U1L9e516ZnZn"},"source":["## Using the hadoop streaming with python\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BKP8O_z1bUIx"},"source":["So looks like that hadoop provides the ability to run mapreduce tasks for non java tasks as well.\n","It is via an API called **hadoop-streaming**\n","\n","Now the hadoop-streaming is itself a jar file, so we would again run some line like: \n","```\n","hadoop jar /usr/local/Cellar/hadoop/3.1.0/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar \n","\n","-file /<path_to_mapper>/mapper.py \\\n","-mapper /<path_to_mapper>/mapper.py \\\n","-file /<path_to_reducer>/reducer.py  \\\n","-reducer /<path_to_reducer>/reducer.py  \\\n","-input daily/daily.csv \\\n","-output daily/output\n","```\n","\n","Now to me it looks like that the above line is generaly in the form of\n","\n","```\n","hadoop jar path/to/file/hadoop-streaming.jar [options]\n","```\n","\n","But the issue was that I could not find any such file in the docker build of the hadoop distribution.\n","\n","When searching more in the web for this I could find that \n","it would be in the hadoop distribution \n","\n","> the Hadoop streaming jar is still available in the latest release of EMR Hadoop. Starting with EMR release 4.0.0 it can be found at /usr/lib/hadoop-mapreduce/hadoop-streaming.jar.\n","\n","https://stackoverflow.com/questions/32543734/how-to-find-jar-home-hadoop-contrib-streaming-hadoop-streaming-jar\n","\n","or in \n","\n","> Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer. For example:\n","$HADOOP_HOME/bin/hadoop  jar $HADOOP_HOME/hadoop-streaming.jar \\\n","\n","https://hadoop.apache.org/docs/r1.2.1/streaming.html\n","\n","so I thought I would need to get the original hadoop distirbution.\n","(now all of these troubles may sound laughable for someone with Java expereicne but, these are in fact cnfusing for someone like me)\n","\n","okay now I got the hadoop source from \n","\n","https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.1.4/hadoop-3.1.4-src.tar.gz\n","\n","But it did not have any hadoop-streaming.jar in it. (maybe it needs builidng or something I don't know). But anyway it did not seem usefull.\n","\n","I thought I should use the exact jar file from somewhere, so I could find it here:\n","\n","http://www.java2s.com/Code/Jar/h/Downloadhadoopstreamingjar.htm\n","\n","https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-streaming/3.1.0/\n","\n","now the plan is to copy it into the docker container along with the input options and see if it works. Will see in a bit!\n"]},{"cell_type":"code","metadata":{"id":"Q64TcVvb8q4k"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jZwusdYSlQDr"},"source":["## Before a tutorial example"]},{"cell_type":"markdown","metadata":{"id":"U0Kuby49j-CK"},"source":["We defenitely need more detail on how the hadoop-streaming api works.\n","\n","Some references to look at \n","\n","https://hadoop.apache.org/docs/r1.2.1/streaming.pdf\n","\n","https://hadoop.apache.org/docs/r1.2.1/streaming.html\n","\n","\n","http://www.devx.com/opensource/introduction-to-hadoop-streaming.html\n","\n","\n","But A key question to be addressed is that if we can define a Map Reduce Task in any kind of executable, including python and hadoop will do the job, then what is essentially this mapreduce task.\n","How the hadoop hdfs helps with running this task. \n","Can it be applied on more than one file? \n","how the files and Data in general are organzed and placed in the HDFS?\n"]},{"cell_type":"markdown","metadata":{"id":"Qowa3FChlfUc"},"source":["### What makes a python code a Map/Reduce task?\n"]},{"cell_type":"markdown","metadata":{"id":"DP6wAtc6x9b5"},"source":["Again, the question was how the map reduce works with hadoop. If we can write our own map and reduce  scripts in python how they should look like ?\n","\n","okay I read through the following sources and looked at the python codes to get an idea of what it means to have a map reduce task. The short answer is that the map task produces a set of (key, value)  pairs (which is from processing a chunk of input data, but we can assume that it is all the data for now).\n","and the reduce task is processing and somehow summing up the  outputs of the mappers ( i.e. the set of key, values). And I guess hadoop does some manipulation of the (key, value) paris of all mappers before handing them to reducers."]},{"cell_type":"markdown","metadata":{"id":"-8AB5FZ9w5Hs"},"source":["https://intellipaat.com/blog/tutorial/hadoop-tutorial/mapreduce-in-hadoop/\n","\n","\n","https://intellipaat.com/blog/tutorial/hadoop-tutorial/hadoop-streaming/\n","\n","https://www.youtube.com/watch?v=qskfdqsK9fk&feature=emb_logo&ab_channel=Intellipaat\n","\n","https://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/\n","\n","\n","\n","https://cwiki.apache.org/confluence/display/HADOOP2/HadoopMapReduce#app-switcher\n","\n","\n","https://en.wikipedia.org/wiki/MapReduce\n","\n","\n","\n","https://cwiki.apache.org/confluence/display/HADOOP2/HadoopStreaming\n","\n","\n","\n","https://blog.matthewrathbone.com/2013/11/17/python-map-reduce-on-hadoop-a-beginners-tutorial.html"]},{"cell_type":"markdown","metadata":{"id":"dudWBSFq0Z_b"},"source":["Now among the source I could find in a shallow search, this one here was actually made better understand what is going on \n","\n","https://intellipaat.com/blog/tutorial/hadoop-tutorial/mapreduce-in-hadoop/\n","\n","and this one has a good example in action \n","\n","https://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/\n","\n","\n","which I will go through it more, in the rest of this.\n"]},{"cell_type":"markdown","metadata":{"id":"g7AjLTK8a3Ev"},"source":["#Writing a Map Reduce task in python"]},{"cell_type":"code","metadata":{"id":"vL6aQiF0taPs","executionInfo":{"status":"ok","timestamp":1602266958477,"user_tz":420,"elapsed":164,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"d815d8bb-ddf6-43aa-fbd7-5f80b1ba245e","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["docker-hadoop\t\t\t\t     hadoop_source\r\n","hadoop-mapreduce-examples-2.7.1-sources.jar  python_hadoop_example\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y2gYvnXE4CkY","executionInfo":{"status":"ok","timestamp":1602289190513,"user_tz":420,"elapsed":37,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"400b45d6-650f-4250-aef5-2ef1a438f2a0","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd python_hadoop_examples/example2"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/media/hossein/HDD/gdrive/Active_projects/my_dfs/python_hadoop_examples/example2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vRAe8k9rjiOq"},"source":["### Word Count example\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"onYbJP2Z5nzS"},"source":["https://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/"]},{"cell_type":"markdown","metadata":{"id":"dn8xmyCO5tPs"},"source":["\n","\n","```\n","#!/usr/bin/env python3\n","\"\"\"mapper.py\"\"\"\n","\n","import sys\n","\n","# input comes from STDIN (standard input)\n","for line in sys.stdin:\n","    # remove leading and trailing whitespace\n","    line = line.strip()\n","    # split the line into words\n","    words = line.split()\n","    # increase counters\n","    for word in words:\n","        # write the results to STDOUT (standard output);\n","        # what we output here will be the input for the\n","        # Reduce step, i.e. the input for reducer.py\n","        #\n","        # tab-delimited; the trivial word count is 1\n","        print '%s\\t%s' % (word, 1)```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KEjSynT951_-"},"source":["\n","\n","```\n","#!/usr/bin/env python3\n","\"\"\"reducer.py\"\"\"\n","\n","from operator import itemgetter\n","import sys\n","\n","current_word = None\n","current_count = 0\n","word = None\n","\n","# input comes from STDIN\n","for line in sys.stdin:\n","    # remove leading and trailing whitespace\n","    line = line.strip()\n","\n","    # parse the input we got from mapper.py\n","    word, count = line.split('\\t', 1)\n","\n","    # convert count (currently a string) to int\n","    try:\n","        count = int(count)\n","    except ValueError:\n","        # count was not a number, so silently\n","        # ignore/discard this line\n","        continue\n","\n","    # this IF-switch only works because Hadoop sorts map output\n","    # by key (here: word) before it is passed to the reducer\n","    if current_word == word:\n","        current_count += count\n","    else:\n","        if current_word:\n","            # write result to STDOUT\n","            print '%s\\t%s' % (current_word, current_count)\n","        current_count = count\n","        current_word = word\n","\n","# do not forget to output the last word if needed!\n","if current_word == word:\n","    print '%s\\t%s' % (current_word, current_count)```\n","\n"]},{"cell_type":"code","metadata":{"id":"ZLCWl1O26B-g"},"source":["!ls\n","!wget http://www.gutenberg.org/ebooks/20417.txt.utf-8\n","!wget http://www.gutenberg.org/files/5000/5000-8.txt\n","!wget http://www.gutenberg.org/files/4300/4300-0.txt\n","!ls\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nZgv7zmdBZ4h"},"source":["!chmod +x ./mapper.py\n","!chmod +x ./reducer.py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ckMhKc-mRZpC","executionInfo":{"status":"ok","timestamp":1602296092769,"user_tz":420,"elapsed":173,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"374a0623-388f-4aa5-b9fc-4c9d6f78848c","colab":{"base_uri":"https://localhost:8080/"}},"source":["!echo \"foo foo quux labs foo bar quux\" | ./mapper.py\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["foo\t1\r\n","foo\t1\r\n","quux\t1\r\n","labs\t1\r\n","foo\t1\r\n","bar\t1\r\n","quux\t1\r\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0aqxQsYrSGNR"},"source":["if it does not work, it could be due to the proper configuration of the python for ubuntu, this could help\n","\n","http://openbookproject.net/thinkcs/python/english3e/app_c.html\n","\n","in my case, it was only that I needed to replace the python in the first line of mapper and reducer to python3\n","\n"]},{"cell_type":"code","metadata":{"id":"75tZHN1wTM1C","executionInfo":{"status":"ok","timestamp":1602296349452,"user_tz":420,"elapsed":153,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"a99aa707-ba6e-4309-b4f9-fc4adb1089b2","colab":{"base_uri":"https://localhost:8080/"}},"source":["!echo \"foo foo quux labs foo bar quux\" | ./mapper.py | sort -k1,1 | ./reducer.py\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["bar\t1\r\n","foo\t3\r\n","labs\t1\r\n","quux\t2\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"33TkNlWjToYh"},"source":["!cat ./4300-0.txt | ./mapper.py"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DV7dUXEaBRxF"},"source":["okay next step would be to run the codes in hadoop.\n","okay first we copy the data and all into the container."]},{"cell_type":"code","metadata":{"id":"IaOhYFxaUbgK"},"source":["!docker cp . namenode:media/example2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NfaR9hhMVQ8a"},"source":["we should repeat the steps for checking the py codes in the container as well.\n"]},{"cell_type":"markdown","metadata":{"id":"en-AA0afYEWv"},"source":["okay now my container does not recognize either python of python3.\n","\n","so the stuff in the link above does not work\n","\n","\n","\n","export PATH=$PATH:/usr/lib/python3\n","\n","okay so none of the games I played with the  path variables did work. looked like to me tht there is no python installed on the image.\n","\n","So after a long and painfull course of actions I installed python on the image.\n","we need to change the dockerfile for the nodename and then change the docker compose file.\n","\n","now we have the python on the container.\n","but we need to copy files again.\n"]},{"cell_type":"code","metadata":{"id":"RoQfzCcim9Yn","executionInfo":{"status":"ok","timestamp":1602301454900,"user_tz":420,"elapsed":131,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"9b3fa79c-a5e4-4eee-d061-1ffc636d6eec","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["20417.txt.utf-8  4300-0.txt  5000-8.txt  mapper.py  reducer.py\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XCrSsjnem_TI"},"source":["!docker cp . namenode:media/example2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eG6rFW11nRAP"},"source":["and .. yes... the mapper.py works in the container.\n"]},{"cell_type":"markdown","metadata":{"id":"F6Q-a2UnoN57"},"source":["```\n","hadoop hdfs -copyFromLocal /media/example2 /user/gutenberg\n","```\n","```\n","WARNING: Use of this script to execute dfs is deprecated.\n","WARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n","\n","2020-10-10 03:50:58,131 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n","2020-10-10 03:50:58,240 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n","2020-10-10 03:50:58,269 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n","2020-10-10 03:50:58,304 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n","2020-10-10 03:50:58,332 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n","root@241cc6651043:/media/example2# \n","```\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"y6ePYkFwqglW"},"source":["okay so this above command looks like did not work entirely. so at the end I used the following\n","\n","```\n","hdfs dfs -copyFromLocal /media/example2 gutenberg\n","```\n","\n","should learn some more on this hdfs manipulation\n","\n","okay now we have the mapper code we have the data on hdfs, now we should copy the streemin jar file.\n"]},{"cell_type":"code","metadata":{"id":"vzK4y9lAqfRH","executionInfo":{"status":"ok","timestamp":1602303237329,"user_tz":420,"elapsed":286,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"ad1a6fa4-2961-477f-c424-79d0e3cd197c","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls\n","%cd ../hadoop_source/\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["example1  example2\n","/media/hossein/HDD/gdrive/Active_projects/my_dfs/hadoop_source\n","hadoop-3.1.4-src\t hadoop-streaming-2.1.0-beta.jar\n","hadoop-3.1.4-src.tar.gz  hadoop-streaming-2.1.0-beta.jar.zip\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yPT7gxX7ty6T"},"source":["!docker cp ./hadoop-streaming-2.1.0-beta.jar namenode:hadoop-streaming-2.1.0-beta.jar"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vDu1Nyi14SSM"},"source":["### Currency example"]},{"cell_type":"markdown","metadata":{"id":"LvkFZE5p48KO"},"source":["https://medium.com/@rrfd/your-first-map-reduce-using-hadoop-with-python-and-osx-ca3b6f3dfe78"]},{"cell_type":"markdown","metadata":{"id":"H_Ipl_PWumhe"},"source":["Let’s look at our mapper.py and make sure to include #!/usr/bin/python at the top of you script. The #! is called a shebang and allows your script to be executed like a standalone executable without typing python in front of it. I’ve included it to ensure our script runs, because sometimes Hadoop can be fussy with executables."]},{"cell_type":"markdown","metadata":{"id":"2jl4s2_2u6wx"},"source":["```\n","#!/usr/bin/python\n","# The Mapper\n","import sys\n","import csv\n","# Set local variables\n","iteration = 0\n","currentCountry = None\n","previousCountry = None\n","currentFx = None\n","previousFx = None\n","percentChange = None\n","currentKey = None\n","fxMap = []\n","# print \"Starting mapper.py\"\n","infile = sys.stdin\n","next(infile) # skip first line of input file\n","for line in infile:\n","line = line.strip()\n","    line = line.split(',', 2)\n","try:\n","        # Get data from line\n","        currentCountry = line[1].rstrip()\n","        if len(line[2]) == 0:\n","            continue\n","        currentFx = float(line[2])\n","if currentCountry != previousCountry:\n","            previousCountry = currentCountry\n","            previousFx = currentFx\n","            previousLine = line\n","            continue\n","# If country same as previous, add to map\n","        elif currentCountry == previousCountry:\n","            percentChange = ((currentFx - previousFx) / previousFx) * 100.00\n","            percentChange = round(percentChange, 2)\n","            percentChange = percentChange\n","currentKey = \"%s: %6.2f%%\" % (currentCountry, percentChange)\n","# Set the array with tuple keys\n","            fxMap.append(tuple([currentKey, 1]))\n","# Update Values\n","        previousCountry = currentCountry\n","        previousFx = currentFx\n","        previousLine = line\n","# Uncomment if you want to see the output\n","#         if iteration % 50000 == 0:\n","#             print \"Current iteration is %d\" % iteration\n","#         iteration += 1\n","# Handle unexpected errors\n","    except Exception as e:\n","        template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n","        message = template.format(type(e).__name__, e.args)\n","        print \"currentFx: %.2f previousFx: %.2f\" % (currentFx, previousFx)\n","        print message\n","        sys.exit(0)\n","#\n","# print \"mapper.py has completed with %d iterations\" % (iteration - 1)\n","# Show the returned values\n","for i in sorted(fxMap):\n","    print \"%-20s - %d\" % (i[0], i[1])\n","```"]},{"cell_type":"markdown","metadata":{"id":"sWP_Rh0VZ2d2"},"source":["So looks like we ccan "]},{"cell_type":"markdown","metadata":{"id":"i7pNhUuMrAeb"},"source":["# Runnig the hadoop streaming job:\n"]},{"cell_type":"code","metadata":{"id":"N_6TwuVU-Ct8"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m4BWPXVP-Dzo"},"source":["## First try of the steam task:"]},{"cell_type":"markdown","metadata":{"id":"SKb9UcPKuESd"},"source":["```\n","hadoop jar hadoop-streaming-2.1.0-beta.jar \\\n","-file /media/example2/mapper.py    -mapper /media/example2/mapper.py \\\n","-file /media/example2/reducer.py   -reducer /media/example2/reducer.py \\\n","-input gutenberg/* -output gutenberg-output\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"ytBVZf6FiAo2"},"source":["Okay so this job failed so misrably.\n","\n","with a very looooongggg1 error message:\n","\n","```\n","root@241cc6651043:/# hadoop jar hadoop-streaming-2.1.0-beta.jar \\\n","> -file /media/example2/mapper.py    -mapper /media/example2/mapper.py \\\n","> -file /media/example2/reducer.py   -reducer /media/example2/reducer.py \\\n","> -input gutenberg/* -output gutenberg-output\n","2020-10-10 17:15:38,928 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n","packageJobJar: [/media/example2/mapper.py, /media/example2/reducer.py, /tmp/hadoop-unjar1909972349476209814/] [] /tmp/streamjob6870132652541640349.jar tmpDir=null\n","2020-10-10 17:15:39,676 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.20.0.7:8032\n","2020-10-10 17:15:39,817 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.20.0.8:10200\n","2020-10-10 17:15:39,848 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.20.0.7:8032\n","2020-10-10 17:15:39,848 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.20.0.8:10200\n","2020-10-10 17:15:39,993 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1602301440091_0001\n","2020-10-10 17:15:40,085 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n","2020-10-10 17:15:40,180 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n","2020-10-10 17:15:40,216 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n","2020-10-10 17:15:40,280 INFO mapred.FileInputFormat: Total input files to process : 5\n","2020-10-10 17:15:40,329 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n","2020-10-10 17:15:40,354 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n","2020-10-10 17:15:40,370 INFO mapreduce.JobSubmitter: number of splits:5\n","2020-10-10 17:15:40,483 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n","2020-10-10 17:15:40,516 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1602301440091_0001\n","2020-10-10 17:15:40,516 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2020-10-10 17:15:40,648 INFO conf.Configuration: resource-types.xml not found\n","2020-10-10 17:15:40,648 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n","2020-10-10 17:15:40,939 INFO impl.YarnClientImpl: Submitted application application_1602301440091_0001\n","2020-10-10 17:15:40,965 INFO mapreduce.Job: The url to track the job: http://resourcemanager:8088/proxy/application_1602301440091_0001/\n","2020-10-10 17:15:40,966 INFO mapreduce.Job: Running job: job_1602301440091_0001\n","2020-10-10 17:15:47,111 INFO mapreduce.Job: Job job_1602301440091_0001 running in uber mode : false\n","2020-10-10 17:15:47,113 INFO mapreduce.Job:  map 0% reduce 0%\n","2020-10-10 17:15:51,185 INFO mapreduce.Job: Task Id : attempt_1602301440091_0001_m_000000_0, Status : FAILED\n","Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 127\n","        at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n","        at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n","        at org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n","        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n","        at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n","        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n","        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n","        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n","        at java.security.AccessController.doPrivileged(Native Method)\n","        at javax.security.auth.Subject.doAs(Subject.java:422)\n","        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n","        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n","\n","2020-10-10 17:15:52,218 INFO mapreduce.Job: Task Id : attempt_1602301440091_0001_m_000001_0, Status : FAILED\n","Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 127\n","        at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n","        at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n","        at org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n","        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n","        at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n","        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n","        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n","        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n","        at java.security.AccessController.doPrivileged(Native Method)\n","        at javax.security.auth.Subject.doAs(Subject.java:422)\n","        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n","        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n","\n","2020-10-10 17:15:53,229 INFO mapreduce.Job: Task Id : attempt_1602301440091_0001_m_000002_0, Status : FAILED\n","Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 127\n","        at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n","        at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n","        at org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n","        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n","        at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n","        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n","        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n","        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n","        at java.security.AccessController.doPrivileged(Native Method)\n","        at javax.security.auth.Subject.doAs(Subject.java:422)\n","        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n","        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n","\n","2020-10-10 17:15:54,235 INFO mapreduce.Job: Task Id : attempt_1602301440091_0001_m_000000_1, Status : FAILED\n","Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 127\n","        at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n","        at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n","        at org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n","        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n","        at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n","        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n","        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n","        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n","        at java.security.AccessController.doPrivileged(Native Method)\n","        at javax.security.auth.Subject.doAs(Subject.java:422)\n","        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n","        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n","\n","2020-10-10 17:15:56,245 INFO mapreduce.Job: Task Id : attempt_1602301440091_0001_m_000001_1, Status : FAILED\n","Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 127\n","        at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n","        at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n","        at org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n","        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n","        at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n","        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n","        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n","        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n","        at java.security.AccessController.doPrivileged(Native Method)\n","        at javax.security.auth.Subject.doAs(Subject.java:422)\n","        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n","        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n","\n","2020-10-10 17:15:58,257 INFO mapreduce.Job: Task Id : attempt_1602301440091_0001_m_000002_1, Status : FAILED\n","Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 127\n","        at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n","        at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n","        at org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n","        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n","        at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n","        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n","        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n","        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n","        at java.security.AccessController.doPrivileged(Native Method)\n","        at javax.security.auth.Subject.doAs(Subject.java:422)\n","        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n","        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n","\n","2020-10-10 17:15:59,267 INFO mapreduce.Job: Task Id : attempt_1602301440091_0001_m_000000_2, Status : FAILED\n","Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 127\n","        at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n","        at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n","        at org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n","        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n","        at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n","        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n","        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n","        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n","        at java.security.AccessController.doPrivileged(Native Method)\n","        at javax.security.auth.Subject.doAs(Subject.java:422)\n","        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n","        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n","\n","2020-10-10 17:16:00,278 INFO mapreduce.Job: Task Id : attempt_1602301440091_0001_m_000001_2, Status : FAILED\n","Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 127\n","        at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n","        at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n","        at org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n","        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n","        at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n","        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n","        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n","        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n","        at java.security.AccessController.doPrivileged(Native Method)\n","        at javax.security.auth.Subject.doAs(Subject.java:422)\n","        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n","        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n","\n","2020-10-10 17:16:01,287 INFO mapreduce.Job: Task Id : attempt_1602301440091_0001_m_000002_2, Status : FAILED\n","Error: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 127\n","        at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:320)\n","        at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:533)\n","        at org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:130)\n","        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:61)\n","        at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n","        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)\n","        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)\n","        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n","        at java.security.AccessController.doPrivileged(Native Method)\n","        at javax.security.auth.Subject.doAs(Subject.java:422)\n","        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\n","        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)\n","\n","2020-10-10 17:16:03,306 INFO mapreduce.Job:  map 100% reduce 100%\n","2020-10-10 17:16:03,325 INFO mapreduce.Job: Job job_1602301440091_0001 failed with state FAILED due to: Task failed task_1602301440091_0001_m_000000\n","Job failed as tasks failed. failedMaps:1 failedReduces:0 killedMaps:0 killedReduces: 0\n","\n","2020-10-10 17:16:03,408 INFO mapreduce.Job: Counters: 14\n","        Job Counters \n","                Failed map tasks=10\n","                Killed map tasks=4\n","                Killed reduce tasks=1\n","                Launched map tasks=12\n","                Other local map tasks=9\n","                Rack-local map tasks=3\n","                Total time spent by all maps in occupied slots (ms)=83972\n","                Total time spent by all reduces in occupied slots (ms)=0\n","                Total time spent by all map tasks (ms)=20993\n","                Total vcore-milliseconds taken by all map tasks=20993\n","                Total megabyte-milliseconds taken by all map tasks=85987328\n","        Map-Reduce Framework\n","                CPU time spent (ms)=0\n","                Physical memory (bytes) snapshot=0\n","                Virtual memory (bytes) snapshot=0\n","2020-10-10 17:16:03,408 ERROR streaming.StreamJob: Job not Successful!\n","Streaming Command Failed!\n","\n","```"]},{"cell_type":"markdown","metadata":{"id":"5aSWc3zTq8V4"},"source":["## debugging of the streaming failed job:"]},{"cell_type":"markdown","metadata":{"id":"rGgeFEFgrUz_"},"source":["okay so I had a few thoughts on what could be the cause:\n","  * something about using -file instead of -files that showed in the message\n","  * something wrong on the mapper or reducer codes\n","  * something wrong on the streaming jar file especially that it was beta ver\n","  * something to look for on error code 127\n","\n","some people said that this error code is mostly due to some error in execuation of python codes by hadoop.\n","the first suggestion was to look at the pyhon 2 and 3 differences such as for the print statements.\n","The other one which I think is more probable is due to the path of the python.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"pqEx-cABPe2L","executionInfo":{"status":"ok","timestamp":1602379212505,"user_tz":420,"elapsed":248,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"33d330c9-9acf-4c22-f66d-45e9b15610be","colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["!ls\n","%cd ..\n","!ls"],"execution_count":3,"outputs":[{"output_type":"stream","text":["main  README.md\n","/media/hossein/HDD/gdrive/SkillsLearningTraining\n"," Battery\t    Energy_Markets\t  Optimization\n","'Colab Notebooks'   General_EE_topics\t  PES_topics\n"," courses\t    General_good_reads\t  Programming\n"," Data_science\t    Hot_research_works\t  Skills_and_training\n"," docker-tutorials   Operations_research   tutorials-python\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hcioafZJPp5I","executionInfo":{"status":"ok","timestamp":1602379281332,"user_tz":420,"elapsed":130,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"59a23cf3-7f98-4b88-9062-5782f2511b8a","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["%cd ../Active_projects/my_dfs/\n","!ls"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/media/hossein/HDD/gdrive/Active_projects/my_dfs\n","docker-hadoop\t\t\t\t     hadoop_source\r\n","hadoop-mapreduce-examples-2.7.1-sources.jar  python_hadoop_examples\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7y16j7FbP6E3","executionInfo":{"status":"ok","timestamp":1602379309189,"user_tz":420,"elapsed":156,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"af778dcd-5fcc-4496-dd35-b7ba7da24f6b","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["%cd hadoop_source/\n","!ls"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/media/hossein/HDD/gdrive/Active_projects/my_dfs/hadoop_source\n","hadoop-3.1.4-src\t\t hadoop-streaming-2.1.0-beta.jar.zip\r\n","hadoop-3.1.4-src.tar.gz\t\t hadoop-streaming-3.1.0.jar\r\n","hadoop-streaming-2.1.0-beta.jar\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QxTGOr3oP_hr","executionInfo":{"status":"ok","timestamp":1602379385587,"user_tz":420,"elapsed":232,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}}},"source":["!docker cp ./hadoop-streaming-3.1.0.jar namenode:hadoop-streaming-3.1.0.jar"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"EZwiXq5aR-rd","executionInfo":{"status":"ok","timestamp":1602383457863,"user_tz":420,"elapsed":245,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}}},"source":["!docker cp hadoop_source/hadoop-streaming-2.1.0-beta.jar namenode:hadoop-streaming-2.1.0-beta.jar\n"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VDmwgk4la7Wr"},"source":["okay so I thought i need to install python on all worker nodes\n","so runnig it again"]},{"cell_type":"code","metadata":{"id":"wvitXbHobJxe","executionInfo":{"status":"ok","timestamp":1602383425080,"user_tz":420,"elapsed":123,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"0883788b-5b02-4a83-8505-ea404d68ea76","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!ls"],"execution_count":14,"outputs":[{"output_type":"stream","text":["docker-hadoop\t\t\t\t     hadoop_source\r\n","hadoop-mapreduce-examples-2.7.1-sources.jar  python_hadoop_examples\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mHBS0cPwbQYD","executionInfo":{"status":"ok","timestamp":1602382292783,"user_tz":420,"elapsed":120,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"99aede7e-f667-4408-d5e8-64d28ee6e4c4","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["%cd ..\n","!ls"],"execution_count":10,"outputs":[{"output_type":"stream","text":["/media/hossein/HDD/gdrive/Active_projects/my_dfs\n","docker-hadoop\t\t\t\t     hadoop_source\r\n","hadoop-mapreduce-examples-2.7.1-sources.jar  python_hadoop_examples\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DGSFCYA0bXoc","executionInfo":{"status":"ok","timestamp":1602383478963,"user_tz":420,"elapsed":218,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}}},"source":["!docker cp ./python_hadoop_examples/example2/ namenode:media/example2"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cZt54MAzgwJK"},"source":["Actually still failed with python on both name nodes and data nodes.\n","SO this time installed python on all the nodes!\n","and repeating all above again.\n"]},{"cell_type":"markdown","metadata":{"id":"MpKC_QgFg9H_"},"source":["### And Finally!\n"]},{"cell_type":"markdown","metadata":{"id":"1UTZNEExhJwM"},"source":["So yess!! this time worked!\n","\n","speciall thanks to\n","\n","https://github.com/Yelp/mrjob/issues/1564\n","\n","\n","Well of course the parts about the path to python and changing the commands etc. did not work, but he was correct that I need python on my Cluster and not just the interface node.\n","\n","These ones were also somewhat related:\n","\n","https://stackoverflow.com/questions/48916243/python-hadoop-streaming-on-windows-script-not-a-valid-win32-application\n","\n","https://stackoverflow.com/questions/52904865/running-a-hadoop-streaming-and-mapreduce-job-pipemapred-waitoutputthreads-s\n","\n","\n","https://stackoverflow.com/questions/43048654/hadoop-python-subprocess-failed-with-code-127\n","\n","\n","Sp here is what the message looks like"]},{"cell_type":"markdown","metadata":{"id":"aQUKHJ2JhBZ7"},"source":["```\n","hadoop jar hadoop-streaming-2.1.0-beta.jar -file /media/example2/mapper.py    -mapper /media/example2/mapper.py -file /media/example2/reducer.py   -reducer /media/example2/reducer.py -input gutenberg/* -output gutenberg-output12\n","2020-10-11 02:33:05,572 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n","packageJobJar: [/media/example2/mapper.py, /media/example2/reducer.py, /tmp/hadoop-unjar4236337157738795401/] [] /tmp/streamjob5367433111899547056.jar tmpDir=null\n","2020-10-11 02:33:06,246 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.20.0.8:8032\n","2020-10-11 02:33:06,368 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.20.0.6:10200\n","2020-10-11 02:33:06,392 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.20.0.8:8032\n","2020-10-11 02:33:06,392 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.20.0.6:10200\n","2020-10-11 02:33:06,530 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1602383551624_0001\n","2020-10-11 02:33:06,611 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n","2020-10-11 02:33:06,698 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n","2020-10-11 02:33:06,736 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n","2020-10-11 02:33:06,811 INFO mapred.FileInputFormat: Total input files to process : 10\n","2020-10-11 02:33:06,855 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n","2020-10-11 02:33:06,882 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n","2020-10-11 02:33:06,898 INFO mapreduce.JobSubmitter: number of splits:10\n","2020-10-11 02:33:06,987 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n","2020-10-11 02:33:07,008 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1602383551624_0001\n","2020-10-11 02:33:07,009 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2020-10-11 02:33:07,165 INFO conf.Configuration: resource-types.xml not found\n","2020-10-11 02:33:07,165 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n","2020-10-11 02:33:07,450 INFO impl.YarnClientImpl: Submitted application application_1602383551624_0001\n","2020-10-11 02:33:07,486 INFO mapreduce.Job: The url to track the job: http://resourcemanager:8088/proxy/application_1602383551624_0001/\n","2020-10-11 02:33:07,488 INFO mapreduce.Job: Running job: job_1602383551624_0001\n","2020-10-11 02:33:12,560 INFO mapreduce.Job: Job job_1602383551624_0001 running in uber mode : false\n","2020-10-11 02:33:12,562 INFO mapreduce.Job:  map 0% reduce 0%\n","2020-10-11 02:33:18,681 INFO mapreduce.Job:  map 10% reduce 0%\n","2020-10-11 02:33:19,687 INFO mapreduce.Job:  map 20% reduce 0%\n","2020-10-11 02:33:20,701 INFO mapreduce.Job:  map 30% reduce 0%\n","2020-10-11 02:33:23,714 INFO mapreduce.Job:  map 40% reduce 0%\n","2020-10-11 02:33:24,721 INFO mapreduce.Job:  map 50% reduce 0%\n","2020-10-11 02:33:25,724 INFO mapreduce.Job:  map 60% reduce 0%\n","2020-10-11 02:33:26,728 INFO mapreduce.Job:  map 70% reduce 0%\n","2020-10-11 02:33:27,741 INFO mapreduce.Job:  map 80% reduce 0%\n","2020-10-11 02:33:28,751 INFO mapreduce.Job:  map 90% reduce 0%\n","2020-10-11 02:33:30,764 INFO mapreduce.Job:  map 100% reduce 0%\n","2020-10-11 02:33:34,784 INFO mapreduce.Job:  map 100% reduce 100%\n","2020-10-11 02:33:34,794 INFO mapreduce.Job: Job job_1602383551624_0001 completed successfully\n","2020-10-11 02:33:34,857 INFO mapreduce.Job: Counters: 55\n","        File System Counters\n","                FILE: Number of bytes read=547367\n","                FILE: Number of bytes written=4176375\n","                FILE: Number of read operations=0\n","                FILE: Number of large read operations=0\n","                FILE: Number of write operations=0\n","                HDFS: Number of bytes read=7384087\n","                HDFS: Number of bytes written=894164\n","                HDFS: Number of read operations=35\n","                HDFS: Number of large read operations=0\n","                HDFS: Number of write operations=2\n","                HDFS: Number of bytes read erasure-coded=0\n","        Job Counters \n","                Killed map tasks=1\n","                Launched map tasks=10\n","                Launched reduce tasks=1\n","                Rack-local map tasks=10\n","                Total time spent by all maps in occupied slots (ms)=86796\n","                Total time spent by all reduces in occupied slots (ms)=28816\n","                Total time spent by all map tasks (ms)=21699\n","                Total time spent by all reduce tasks (ms)=3602\n","                Total vcore-milliseconds taken by all map tasks=21699\n","                Total vcore-milliseconds taken by all reduce tasks=3602\n","                Total megabyte-milliseconds taken by all map tasks=88879104\n","                Total megabyte-milliseconds taken by all reduce tasks=29507584\n","        Map-Reduce Framework\n","                Map input records=157624\n","                Map output records=1260602\n","                Map output bytes=9677088\n","                Map output materialized bytes=1067856\n","                Input split bytes=1083\n","                Combine input records=0\n","                Combine output records=0\n","                Reduce input groups=82604\n","                Reduce shuffle bytes=1067856\n","                Reduce input records=1260602\n","                Reduce output records=82604\n","                Spilled Records=2521204\n","                Shuffled Maps =10\n","                Failed Shuffles=0\n","                Merged Map outputs=10\n","                GC time elapsed (ms)=493\n","                CPU time spent (ms)=12810\n","                Physical memory (bytes) snapshot=3636121600\n","                Virtual memory (bytes) snapshot=59547975680\n","                Total committed heap usage (bytes)=4254072832\n","                Peak Map Physical memory (bytes)=364384256\n","                Peak Map Virtual memory (bytes)=5112508416\n","                Peak Reduce Physical memory (bytes)=264335360\n","                Peak Reduce Virtual memory (bytes)=8454361088\n","        Shuffle Errors\n","                BAD_ID=0\n","                CONNECTION=0\n","                IO_ERROR=0\n","                WRONG_LENGTH=0\n","                WRONG_MAP=0\n","                WRONG_REDUCE=0\n","        File Input Format Counters \n","                Bytes Read=7383004\n","        File Output Format Counters \n","                Bytes Written=894164\n","2020-10-11 02:33:34,857 INFO streaming.StreamJob: Output directory: gutenberg-output12\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"-a_JqzIsk4pW"},"source":["can check out the results by \n","```\n","hdfs dfs -cat gutenberg-output12/part-00000\n","hdfs dfs -ls gutenberg-output12\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"g6gUCuDWq8eW"},"source":["### hadoop streaming run options"]},{"cell_type":"markdown","metadata":{"id":"ES8fgn3Ylq_n"},"source":["```\n","hadoop jar hadoop-*streaming*.jar -files /media/example2/mapper.py    -mapper /media/example2/mapper.py -files /media/example2/reducer.py   -reducer /media/example2/reducer.py -input gutenberg/* -output gutenberg-output\n","2020-10-10 17:29:32,387 ERROR streaming.StreamJob: Unrecognized option: -files\n","Usage: $HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar [options]\n","Options:\n","  -input          <path> DFS input file(s) for the Map step.\n","  -output         <path> DFS output directory for the Reduce step.\n","  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n","  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n","  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n","  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n","                  Deprecated. Use generic option \"-files\" instead.\n","  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n","                  Optional. The input format class.\n","  -outputformat   <TextOutputFormat(default)|JavaClassName>\n","                  Optional. The output format class.\n","  -partitioner    <JavaClassName>  Optional. The partitioner class.\n","  -numReduceTasks <num> Optional. Number of reduce tasks.\n","  -inputreader    <spec> Optional. Input recordreader spec.\n","  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n","  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n","  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n","  -io             <identifier> Optional. Format to use for input to and output\n","                  from mapper/reducer commands\n","  -lazyOutput     Optional. Lazily create Output.\n","  -background     Optional. Submit the job and don't wait till it completes.\n","  -verbose        Optional. Print verbose output.\n","  -info           Optional. Print detailed usage.\n","  -help           Optional. Print help message.\n","\n","Generic options supported are:\n","-conf <configuration file>        specify an application configuration file\n","-D <property=value>               define a value for a given property\n","-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n","-jt <local|resourcemanager:port>  specify a ResourceManager\n","-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n","-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n","-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n","\n","The general command line syntax is:\n","command [genericOptions] [commandOptions]\n","\n","\n","For more details about these options:\n","Use $HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar -info\n","\n","Try -help for more information\n","Streaming Command Failed!\n","root@241cc6651043:/# hadoom-streaming.jar -infobash: hadoom-streaming.jar: command not found\n","root@241cc6651043:/# hadoop jar hadoop-streaming.jar -info\n","JAR does not exist or is not a normal file: /hadoop-streaming.jar\n","root@241cc6651043:/# hadoop jar hadoop-*streaming*.jar -info\n","Usage: $HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar [options]\n","Options:\n","  -input          <path> DFS input file(s) for the Map step.\n","  -output         <path> DFS output directory for the Reduce step.\n","  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n","  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n","  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n","  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n","                  Deprecated. Use generic option \"-files\" instead.\n","  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n","                  Optional. The input format class.\n","  -outputformat   <TextOutputFormat(default)|JavaClassName>\n","                  Optional. The output format class.\n","  -partitioner    <JavaClassName>  Optional. The partitioner class.\n","  -numReduceTasks <num> Optional. Number of reduce tasks.\n","  -inputreader    <spec> Optional. Input recordreader spec.\n","  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n","  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n","  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n","  -io             <identifier> Optional. Format to use for input to and output\n","                  from mapper/reducer commands\n","  -lazyOutput     Optional. Lazily create Output.\n","  -background     Optional. Submit the job and don't wait till it completes.\n","  -verbose        Optional. Print verbose output.\n","  -info           Optional. Print detailed usage.\n","  -help           Optional. Print help message.\n","\n","Generic options supported are:\n","-conf <configuration file>        specify an application configuration file\n","-D <property=value>               define a value for a given property\n","-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n","-jt <local|resourcemanager:port>  specify a ResourceManager\n","-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n","-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n","-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n","\n","The general command line syntax is:\n","command [genericOptions] [commandOptions]\n","\n","\n","Usage tips:\n","In -input: globbing on <path> is supported and can have multiple -input\n","\n","Default Map input format: a line is a record in UTF-8 the key part ends at first\n","  TAB, the rest of the line is the value\n","\n","To pass a Custom input format:\n","  -inputformat package.MyInputFormat\n","\n","Similarly, to pass a custom output format:\n","  -outputformat package.MyOutputFormat\n","\n","The files with extensions .class and .jar/.zip, specified for the -file\n","  argument[s], end up in \"classes\" and \"lib\" directories respectively inside\n","  the working directory when the mapper and reducer are run. All other files\n","  specified for the -file argument[s] end up in the working directory when the\n","  mapper and reducer are run. The location of this working directory is\n","  unspecified.\n","\n","To set the number of reduce tasks (num. of output files) as, say 10:\n","  Use -numReduceTasks 10\n","To skip the sort/combine/shuffle/sort/reduce step:\n","  Use -numReduceTasks 0\n","  Map output then becomes a 'side-effect output' rather than a reduce input.\n","  This speeds up processing. This also feels more like \"in-place\" processing\n","  because the input filename and the map input order are preserved.\n","  This is equivalent to -reducer NONE\n","\n","To speed up the last maps:\n","  -D mapreduce.map.speculative=true\n","To speed up the last reduces:\n","  -D mapreduce.reduce.speculative=true\n","To name the job (appears in the JobTracker Web UI):\n","  -D mapreduce.job.name='My Job'\n","To change the local temp directory:\n","  -D dfs.data.dir=/tmp/dfs\n","  -D stream.tmpdir=/tmp/streaming\n","Additional local temp directories with -jt local:\n","  -D mapreduce.cluster.local.dir=/tmp/local\n","  -D mapreduce.jobtracker.system.dir=/tmp/system\n","  -D mapreduce.cluster.temp.dir=/tmp/temp\n","To treat tasks with non-zero exit status as SUCCEDED:\n","  -D stream.non.zero.exit.is.failure=false\n","Use a custom hadoop streaming build along with standard hadoop install:\n","  $HADOOP_PREFIX/bin/hadoop jar /path/my-hadoop-streaming.jar [...]\\\n","    [...] -D stream.shipped.hadoopstreaming=/path/my-hadoop-streaming.jar\n","For more details about jobconf parameters see:\n","  http://wiki.apache.org/hadoop/JobConfFile\n","To set an environement variable in a streaming command:\n","   -cmdenv EXAMPLE_DIR=/home/example/dictionaries/\n","\n","Shortcut:\n","   setenv HSTREAMING \"$HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar\"\n","\n","Example: $HSTREAMING -mapper \"/usr/local/bin/perl5 filter.pl\"\n","           -file /local/filter.pl -input \"/logs/0604*/*\" [...]\n","  Ships a script, invokes the non-shipped perl interpreter. Shipped files go to\n","  the working directory so filter.pl is found by perl. Input files are all the\n","  daily logs for days in month 2006-04\n","root@241cc6651043:/# \n","```"]},{"cell_type":"markdown","metadata":{"id":"Dw9iAByCjWSO"},"source":[""]}]}