{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"setup_hadoop_cluster_with_docker.ipynb","provenance":[],"collapsed_sections":["SSFabjvxXs8b","yGeLvRvPXn_r"],"authorship_tag":"ABX9TyOaYzAY7NYUjVvs/nKIMdV2"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"mUVe7EzkWXBH","executionInfo":{"status":"ok","timestamp":1602096116318,"user_tz":420,"elapsed":166,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"32240eec-c118-4070-a95c-f8eb19ff1e3c","colab":{"base_uri":"https://localhost:8080/"}},"source":["!docker ps -a"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CONTAINER ID        IMAGE                COMMAND                  CREATED             STATUS                     PORTS                NAMES\r\n","531e654272af        nginx                \"/docker-entrypoint.…\"   12 minutes ago      Up 12 minutes              0.0.0.0:30->80/tcp   myserver\r\n","8641e0a64e03        mytraps:demo         \"/bin/sh /root/traps\"    20 hours ago        Exited (0) 20 hours ago                         mycontain\r\n","f6e8281f6f27        mysql:5.7            \"docker-entrypoint.s…\"   26 hours ago        Exited (0) 3 hours ago                          app_mysql_1\r\n","d2352454e66f        node:12-alpine       \"docker-entrypoint.s…\"   26 hours ago        Exited (1) 3 hours ago                          app_app_1\r\n","bb07ab843c4d        node:12-alpine       \"docker-entrypoint.s…\"   40 hours ago        Created                                         competent_kowalevski\r\n","4493421d5a28        nicolaka/netshoot    \"/bin/bash -l\"           40 hours ago        Exited (0) 40 hours ago                         intelligent_pike\r\n","658bed8ba63f        docker101tutorial    \"/docker-entrypoint.…\"   4 days ago          Exited (0) 3 hours ago                          docker-tutorial2\r\n","11c586eda298        alpine/git           \"git --help\"             5 days ago          Exited (0) 5 days ago                           hossein_repo6\r\n","b5f82006a8af        busybox              \"echo 'hello from bu…\"   5 days ago          Exited (0) 5 days ago                           elated_clarke\r\n","1c45cf2e260d        busybox              \"sh\"                     5 weeks ago         Exited (137) 5 weeks ago                        elated_cray\r\n","ab321a8b3b42        hello-world:latest   \"/hello\"                 5 weeks ago         Exited (0) 5 weeks ago                          blissful_kilby\r\n","ff53f58c645e        hello-world          \"/hello\"                 2 months ago        Exited (0) 2 months ago                         crazy_herschel\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LynD_Rh3Xr_t"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SSFabjvxXs8b"},"source":["# Setting up a Hadoop Cluster Using Docker"]},{"cell_type":"markdown","metadata":{"id":"-8rKcHAYXsTY"},"source":["from : https://clubhouse.io/developer-how-to/how-to-set-up-a-hadoop-cluster-in-docker/\n","\n","To install Hadoop in a Docker container, we need a Hadoop Docker image. To generate the image, we will use the Big Data Europe repository. I\n","\n"]},{"cell_type":"code","metadata":{"id":"ADKJ2PTydB_3","executionInfo":{"status":"ok","timestamp":1602097602410,"user_tz":420,"elapsed":242,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"84c3ed2b-7993-4153-ee3d-0638621d565d","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls\n","%cd Downloads/\n","%mkdir Hadoop_docker_repo\n","%cd Hadoop_docker_repo"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Desktop    Downloads  localperl  Pictures  snap       Videos\r\n","Documents  gopath     Music\t Public    Templates\r\n","/home/hossein/Downloads\n","/home/hossein/Downloads/Hadoop_docker_repo\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B9lmSUvydXJp","executionInfo":{"status":"ok","timestamp":1602097635394,"user_tz":420,"elapsed":1002,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"38833dc4-5bf2-4272-cba6-336867f36e9d","colab":{"base_uri":"https://localhost:8080/"}},"source":["!git clone git@github.com:big-data-europe/docker-hadoop.git\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'docker-hadoop'...\n","remote: Enumerating objects: 539, done.\u001b[K\n","remote: Total 539 (delta 0), reused 0 (delta 0), pack-reused 539\u001b[K\n","Receiving objects: 100% (539/539), 110.91 KiB | 1.50 MiB/s, done.\n","Resolving deltas: 100% (241/241), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XGc_lygDdqYq","executionInfo":{"status":"ok","timestamp":1602097695665,"user_tz":420,"elapsed":143,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"a91b738c-8225-4dd7-8e01-c5b992d54a69","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["docker-hadoop\r\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dOKfmRQtfD0L"},"source":["Once we have the docker-hadoop folder on your local machine, we will need to edit the docker-compose.yml file to enable some listening ports and change where Docker-compose pulls the images from in case we have the images locally already (Docker will attempt to download files and build the images the first time we run, but on subsequent times, we would love to use the already existing images on disk instead of rebuilding everything from scratch again). Open the docker-compose.yml file and replace the content with the following (You can also download or copy and paste from this Github Gist):\n","\n","https://gist.github.com/themonster2015/35cf4252893cdcc831e79e76deb95cdb\n"]},{"cell_type":"code","metadata":{"id":"6QPeE8rIfQrG"},"source":["!docker-compose up -d"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K6p4PXOBf0jU"},"source":["```\n","Creating network \"docker-hadoop_default\" with the default driver\n","Creating volume \"docker-hadoop_hadoop_namenode\" with default driver\n","Creating volume \"docker-hadoop_hadoop_datanode1\" with default driver\n","Creating volume \"docker-hadoop_hadoop_datanode2\" with default driver\n","Creating volume \"docker-hadoop_hadoop_datanode3\" with default driver\n","Creating volume \"docker-hadoop_hadoop_historyserver\" with default driver\n","Building namenode\n","Step 1/10 : FROM bde2020/hadoop-base:2.0.0-hadoop3.2.1-java8\n","2.0.0-hadoop3.2.1-java8: Pulling from bde2020/hadoop-base\n","3192219afd04: Pull complete\n","7127a1d8cced: Pull complete\n","883a89599900: Pull complete\n","77920a3e82af: Pull complete\n","92329e81aec4: Pull complete\n","f373218fec59: Pull complete\n","aa53513fe997: Pull complete\n","8b1800105b98: Pull complete\n","c3a84a3e49c8: Pull complete\n","a65640a64a76: Pull complete\n","Digest: sha256:bd686a92ad0356e3fa30237428c807bb03d1a4e71dceb22338fa67ee5f0a3820\n","Status: Downloaded newer image for bde2020/hadoop-base:2.0.0-hadoop3.2.1-java8\n"," ---> a89a06d383e8\n","Step 2/10 : MAINTAINER Ivan Ermilov <ivan.s.ermilov@gmail.com>\n"," ---> Running in eba881d884a0\n","Removing intermediate container eba881d884a0\n"," ---> 5a4d2f738b21\n","Step 3/10 : HEALTHCHECK CMD curl -f http://localhost:9870/ || exit 1\n"," ---> Running in c5c051562b12\n","Removing intermediate container c5c051562b12\n"," ---> d80b75ad1e63\n","Step 4/10 : ENV HDFS_CONF_dfs_namenode_name_dir=file:///hadoop/dfs/name\n"," ---> Running in fb001d3afa7c\n","Removing intermediate container fb001d3afa7c\n"," ---> 2984c462817a\n","Step 5/10 : RUN mkdir -p /hadoop/dfs/name\n"," ---> Running in 3d8e19e5488a\n","Removing intermediate container 3d8e19e5488a\n"," ---> 4ee1e22946f5\n","Step 6/10 : VOLUME /hadoop/dfs/name\n"," ---> Running in 62ad64d8d078\n","Removing intermediate container 62ad64d8d078\n"," ---> d998948c0a9a\n","Step 7/10 : ADD run.sh /run.sh\n"," ---> 519164753fb8\n","Step 8/10 : RUN chmod a+x /run.sh\n"," ---> Running in ecb2fff0e924\n","Removing intermediate container ecb2fff0e924\n"," ---> f1d1ba31e7df\n","Step 9/10 : EXPOSE 9870\n"," ---> Running in d07c451ede44\n","Removing intermediate container d07c451ede44\n"," ---> 0f3c4468c859\n","Step 10/10 : CMD [\"/run.sh\"]\n"," ---> Running in 97a0b6e317c7\n","Removing intermediate container 97a0b6e317c7\n"," ---> 53c7cb68fb5f\n","\n","Successfully built 53c7cb68fb5f\n","Successfully tagged bde2020/hadoop-namenode:1.1.0-hadoop2.7.1-java8\n","WARNING: Image for service namenode was built because it did not already exist. To rebuild this image you must use `docker-compose build` or `docker-compose up --build`.\n","Building datanode1\n","Step 1/10 : FROM bde2020/hadoop-base:2.0.0-hadoop3.2.1-java8\n"," ---> a89a06d383e8\n","Step 2/10 : MAINTAINER Ivan Ermilov <ivan.s.ermilov@gmail.com>\n"," ---> Using cache\n"," ---> 5a4d2f738b21\n","Step 3/10 : HEALTHCHECK CMD curl -f http://localhost:9864/ || exit 1\n"," ---> Running in 854d057ea5d5\n","Removing intermediate container 854d057ea5d5\n"," ---> 5c8af487144f\n","Step 4/10 : ENV HDFS_CONF_dfs_datanode_data_dir=file:///hadoop/dfs/data\n"," ---> Running in 7ccc96344d09\n","Removing intermediate container 7ccc96344d09\n"," ---> edb8d8d9ad26\n","Step 5/10 : RUN mkdir -p /hadoop/dfs/data\n"," ---> Running in f24b9e5a62d0\n","Removing intermediate container f24b9e5a62d0\n"," ---> 52347daa7d02\n","Step 6/10 : VOLUME /hadoop/dfs/data\n"," ---> Running in d915d13c7ce2\n","Removing intermediate container d915d13c7ce2\n"," ---> 8af6ebfaa01d\n","Step 7/10 : ADD run.sh /run.sh\n"," ---> dd9f2a171751\n","Step 8/10 : RUN chmod a+x /run.sh\n"," ---> Running in 0a6d7e8ba069\n","Removing intermediate container 0a6d7e8ba069\n"," ---> fa0498120b43\n","Step 9/10 : EXPOSE 9864\n"," ---> Running in a26d94002c36\n","Removing intermediate container a26d94002c36\n"," ---> e6262dd1f336\n","Step 10/10 : CMD [\"/run.sh\"]\n"," ---> Running in 5066d457b2d5\n","Removing intermediate container 5066d457b2d5\n"," ---> 6804e14bcc30\n","\n","Successfully built 6804e14bcc30\n","Successfully tagged bde2020/hadoop-datanode:1.1.0-hadoop2.7.1-java8\n","WARNING: Image for service datanode1 was built because it did not already exist. To rebuild this image you must use `docker-compose build` or `docker-compose up --build`.\n","Building nodemanager1\n","Step 1/7 : FROM bde2020/hadoop-base:2.0.0-hadoop3.2.1-java8\n"," ---> a89a06d383e8\n","Step 2/7 : MAINTAINER Ivan Ermilov <ivan.s.ermilov@gmail.com>\n"," ---> Using cache\n"," ---> 5a4d2f738b21\n","Step 3/7 : HEALTHCHECK CMD curl -f http://localhost:8042/ || exit 1\n"," ---> Running in 36973da2e29b\n","Removing intermediate container 36973da2e29b\n"," ---> 45ac22b9536f\n","Step 4/7 : ADD run.sh /run.sh\n"," ---> 8c321d9d5357\n","Step 5/7 : RUN chmod a+x /run.sh\n"," ---> Running in 215a1b604895\n","Removing intermediate container 215a1b604895\n"," ---> ce4f4fb097b1\n","Step 6/7 : EXPOSE 8042\n"," ---> Running in 96b35eb85537\n","Removing intermediate container 96b35eb85537\n"," ---> f38810d8a086\n","Step 7/7 : CMD [\"/run.sh\"]\n"," ---> Running in 2ec0e690689c\n","Removing intermediate container 2ec0e690689c\n"," ---> 182cdf5b2b25\n","\n","Successfully built 182cdf5b2b25\n","Successfully tagged bde2020/hadoop-nodemanager:1.1.0-hadoop2.7.1-java8\n","WARNING: Image for service nodemanager1 was built because it did not already exist. To rebuild this image you must use `docker-compose build` or `docker-compose up --build`.\n","Building historyserver\n","Step 1/10 : FROM bde2020/hadoop-base:2.0.0-hadoop3.2.1-java8\n"," ---> a89a06d383e8\n","Step 2/10 : MAINTAINER Ivan Ermilov <ivan.s.ermilov@gmail.com>\n"," ---> Using cache\n"," ---> 5a4d2f738b21\n","Step 3/10 : HEALTHCHECK CMD curl -f http://localhost:8188/ || exit 1\n"," ---> Running in 91a5b47b986f\n","Removing intermediate container 91a5b47b986f\n"," ---> 668c2a171166\n","Step 4/10 : ENV YARN_CONF_yarn_timeline___service_leveldb___timeline___store_path=/hadoop/yarn/timeline\n"," ---> Running in 43c612c42ac8\n","Removing intermediate container 43c612c42ac8\n"," ---> 1996bb8c140b\n","Step 5/10 : RUN mkdir -p /hadoop/yarn/timeline\n"," ---> Running in 595ad91081f9\n","Removing intermediate container 595ad91081f9\n"," ---> cc5dc1d41af1\n","Step 6/10 : VOLUME /hadoop/yarn/timeline\n"," ---> Running in 45b282cf4f81\n","Removing intermediate container 45b282cf4f81\n"," ---> 65a04d2b6bfb\n","Step 7/10 : ADD run.sh /run.sh\n"," ---> cca2d1fe0371\n","Step 8/10 : RUN chmod a+x /run.sh\n"," ---> Running in fff9066253c4\n","Removing intermediate container fff9066253c4\n"," ---> 0cecb9cb29e3\n","Step 9/10 : EXPOSE 8188\n"," ---> Running in fbbcf1dbcbaa\n","Removing intermediate container fbbcf1dbcbaa\n"," ---> 6e65e84c9215\n","Step 10/10 : CMD [\"/run.sh\"]\n"," ---> Running in afcff4f24b84\n","Removing intermediate container afcff4f24b84\n"," ---> eb6b66eadfd3\n","\n","Successfully built eb6b66eadfd3\n","Successfully tagged bde2020/hadoop-historyserver:1.1.0-hadoop2.7.1-java8\n","WARNING: Image for service historyserver was built because it did not already exist. To rebuild this image you must use `docker-compose build` or `docker-compose up --build`.\n","Building resourcemanager\n","Step 1/7 : FROM bde2020/hadoop-base:2.0.0-hadoop3.2.1-java8\n"," ---> a89a06d383e8\n","Step 2/7 : MAINTAINER Ivan Ermilov <ivan.s.ermilov@gmail.com>\n"," ---> Using cache\n"," ---> 5a4d2f738b21\n","Step 3/7 : HEALTHCHECK CMD curl -f http://localhost:8088/ || exit 1\n"," ---> Running in 5ecaad92f172\n","Removing intermediate container 5ecaad92f172\n"," ---> e33d2a986bdc\n","Step 4/7 : ADD run.sh /run.sh\n"," ---> 7b49eeafa9fc\n","Step 5/7 : RUN chmod a+x /run.sh\n"," ---> Running in 8a69ef48be52\n","Removing intermediate container 8a69ef48be52\n"," ---> 4c7fec173546\n","Step 6/7 : EXPOSE 8088\n"," ---> Running in 940e28d404da\n","Removing intermediate container 940e28d404da\n"," ---> 11ff7b3202cb\n","Step 7/7 : CMD [\"/run.sh\"]\n"," ---> Running in c157976afb7e\n","Removing intermediate container c157976afb7e\n"," ---> 72783c9c2563\n","\n","Successfully built 72783c9c2563\n","Successfully tagged bde2020/hadoop-resourcemanager:1.1.0-hadoop2.7.1-java8\n","WARNING: Image for service resourcemanager was built because it did not already exist. To rebuild this image you must use `docker-compose build` or `docker-compose up --build`.\n","Creating namenode ... done\n","Creating datanode3       ... done\n","Creating datanode2 ... done\n","Creating datanode1 ... done\n","Creating historyserver   ... done\n","Creating resourcemanager ... done\n","Creating nodemanager1    ... done\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"YlRd6gjefUNW"},"source":["With just a single command above, you are setting up a Hadoop cluster with\n","\n","* 3 slaves (datanodes), done\n","*  one HDFS namenode (or the master node to manage the datanodes), done\n","*  one YARN resourcemanager, \n","*  one historyserver and done\n","*  one nodemanager. done\n","\n","( 7\n","containers)\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Njya8juIfkl9","executionInfo":{"status":"ok","timestamp":1602099619457,"user_tz":420,"elapsed":166,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"97de6436-3685-4fd7-dca5-fe33f016b482","colab":{"base_uri":"https://localhost:8080/"}},"source":["!docker ps"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CONTAINER ID        IMAGE                                                    COMMAND                  CREATED             STATUS                    PORTS                               NAMES\r\n","482b48a1c18b        bde2020/hadoop-resourcemanager:1.1.0-hadoop2.7.1-java8   \"/entrypoint.sh /run…\"   25 minutes ago      Up 5 minutes (healthy)    0.0.0.0:8089->8088/tcp              resourcemanager\r\n","066fcae51580        bde2020/hadoop-nodemanager:1.1.0-hadoop2.7.1-java8       \"/entrypoint.sh /run…\"   25 minutes ago      Up 6 minutes (healthy)    8042/tcp                            nodemanager1\r\n","4d67515c5387        bde2020/hadoop-historyserver:1.1.0-hadoop2.7.1-java8     \"/entrypoint.sh /run…\"   25 minutes ago      Up 25 minutes (healthy)   8188/tcp                            historyserver\r\n","106273fd6f8c        bde2020/hadoop-datanode:1.1.0-hadoop2.7.1-java8          \"/entrypoint.sh /run…\"   25 minutes ago      Up 25 minutes (healthy)   9864/tcp                            datanode1\r\n","48298517da16        bde2020/hadoop-datanode:1.1.0-hadoop2.7.1-java8          \"/entrypoint.sh /run…\"   25 minutes ago      Up 25 minutes (healthy)   9864/tcp                            datanode3\r\n","4282dd0df55d        bde2020/hadoop-datanode:1.1.0-hadoop2.7.1-java8          \"/entrypoint.sh /run…\"   25 minutes ago      Up 25 minutes (healthy)   9864/tcp                            datanode2\r\n","bb2d03f39659        bde2020/hadoop-namenode:1.1.0-hadoop2.7.1-java8          \"/entrypoint.sh /run…\"   25 minutes ago      Up 25 minutes (healthy)   9870/tcp, 0.0.0.0:9870->50070/tcp   namenode\r\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AWBc-luilI5W"},"source":[" #Testing  Hadoop cluster\n"]},{"cell_type":"markdown","metadata":{"id":"Kbrg0AKfnayC"},"source":[""]},{"cell_type":"code","metadata":{"id":"lV5ufs2nndZq","executionInfo":{"status":"ok","timestamp":1602100353575,"user_tz":420,"elapsed":312,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"ee775836-cb74-42aa-bf8d-1b9f8ae71c13","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls\n","%cd ..\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/home/hossein/Downloads\n"," applications\t\t\t   'new_config_with_trades (2).json'\r\n"," flyback-karmic_0.6.5-1_all.deb     new_config_with_trades.json\r\n"," flyback-lucid_0.6.5-1_all.deb\t   'optopy_input_test_15092020 (1).json'\r\n"," Hadoop_docker_repo\t\t   'optopy_input_test_15092020 (2).json'\r\n"," itally_poc_opt.tar.gz\t\t    optopy_input_test_15092020.json\r\n","'new_config_with_trades (1).json'\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dzzKypixn5rA","executionInfo":{"status":"ok","timestamp":1602100441038,"user_tz":420,"elapsed":25,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"9497d479-e45e-45d1-eaa9-8de50db06277","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd ../../../media/hossein/HDD/gdrive/Active_projects/my_dfs/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/media/hossein/HDD/gdrive/Active_projects/my_dfs\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F2XxZaLToMjN"},"source":["!docker cp ./hadoop-mapreduce-examples-2.7.1-sources.jar bb2d03f39659:hadoop-mapreduce-examples-2.7.1-sources.jar\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"veJcC_20os1X"},"source":["oky done!"]},{"cell_type":"markdown","metadata":{"id":"yGeLvRvPXn_r"},"source":["##Writing a map reduce"]},{"cell_type":"code","metadata":{"id":"6VIayMI04LuN"},"source":["!docker cp ./python_hadoop_example/daily.csv namenode:media/daily.csv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BfTVdcuAYQre"},"source":["okay so the first example for testing the installation of the hadoop was the  Word count example\n","\n","So looks like that the way hadoop works in that we hand over a mapreduce task (in java) over some data in the hdfs\n","\n","root@namenode:/# hadoop jar hadoop-mapreduce-examples-2.7.1-sources.jar org.apache.hadoop.examples.WordCount input output\n","```\n","root@808dc412974d:/# hdfs dfs -cat output/part-r-00000\n","2020-10-08 22:46:27,742 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n","Docker\t1\n","Hello\t2\n","World\t1\n","root@808dc412974d:/# \n","```\n","\n","if we run the above mapreduce one more time, since the output directory exists we shall get an error.\n","if the output exist the task will run into error!\n","\n","we can remove the output directory via \n","```\n","hadoop fs -rmr /path/to/your/output/\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"uAsocrZi1vkL"},"source":["okay so the reducer code in the link provided is the same as mapper.\n","Anyway, I will drop this step and move to the next.\n","\n","Next, I will run the containers for the hadoop.\n","and get into the namenode container.\n","\n","\n","Make a directory hdfs dfs -mkdir -p /user/<your_user_name>/daily\n","\n","\n"," \n","Don’t make an output directory yet. Hadoop will throw an error when the MapReduce is running if the output directory already exists.\n","\n","okay I tried the following  \n","```\n","hdfs dfs -mkdir -p /user/hossein/daily \n","```\n","but it doesn't work, well, I never setup a hossein user anyway.\n","so the next one I tried is:\n","```\n","hdfs dfs -mkdir -p /user/root/daily \n","```\n","so this one is added.\n","\n","\n","then I checked it bu running this line:\n","\n","\n","```\n","hdfs dfs -ls\n","```\n","\n","the results were this:\n","\n","```\n","Found 3 items\n","drwxr-xr-x   - root supergroup          0 2020-10-08 20:10 daily\n","drwxr-xr-x   - root supergroup          0 2020-10-07 19:48 input\n","drwxr-xr-x   - root supergroup          0 2020-10-08 19:57 output\n","\n","```\n","It is interesting because it still has the inputs from the previous tutorial, although I deleted that container. and set it up again.\n","looks like data are mounted somewhere.\n","now I don't know how this will affect the second part of the recepie that says don't create an output directory, but will see in a bit!\n","\n"]},{"cell_type":"code","metadata":{"id":"C_psfwvQ4Eg9","executionInfo":{"status":"ok","timestamp":1602188503907,"user_tz":420,"elapsed":144,"user":{"displayName":"Seyed Akhavan Hejazi","photoUrl":"","userId":"03644237567046759431"}},"outputId":"02721a25-c2b9-4ea1-fb1b-732eaad25c27","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["docker-hadoop\t\t\t\t     python_hadoop_example\r\n","hadoop-mapreduce-examples-2.7.1-sources.jar\r\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"H1XcpbGC4Kbk"},"source":["Alright done!"]},{"cell_type":"markdown","metadata":{"id":"taG1lIdN50C2"},"source":["Now let’s copy our files to the hdfs from our local machine, onto the cluster with this syntax.\n","\n","> /usr/local/Cellar/hadoop/3.1.0\n","```\n","hdfs dfs -put <path_to_file>/daily.csv daily\n","```\n","\n","Now check what is in the group \n","\n","```\n","hdfs dfs -ls /daily\n","```\n","\n","and if we copied everything over correctly.\n","```\n","root@808dc412974d:/# hdfs dfs -ls daily\n","Found 1 items\n","-rw-r--r--   3 root supergroup    6887034 2020-10-08 20:30 daily/daily.csv\n","root@808dc412974d:/# \n","```"]}]}